<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://hiworldalice.github.io/"/>
  <updated>2019-09-09T08:41:43.967Z</updated>
  <id>https://hiworldalice.github.io/</id>
  
  <author>
    <name>Tnut</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>吴恩达机器学习作业Python实现(五)：偏差和误差</title>
    <link href="https://hiworldalice.github.io/2019/09/09/%E5%81%8F%E5%B7%AE%E4%B8%8E%E8%AF%AF%E5%B7%AE/"/>
    <id>https://hiworldalice.github.io/2019/09/09/偏差与误差/</id>
    <published>2019-09-09T08:31:34.000Z</published>
    <updated>2019-09-09T08:41:43.967Z</updated>
    
    <content type="html"><![CDATA[<p>在训练的前半部分，你将利用水库水位的变化，实现对大坝泄洪量的正则线性回归预测。在接下来的半部分中，你将通过一些来诊断法调试学习算法，并检查偏差和方差的影响。</p><h1 id="正则线性回归"><a href="#正则线性回归" class="headerlink" title="正则线性回归"></a>正则线性回归</h1><p>我们将从可视化包含水位变化(x)和泄洪量(y)的历史记录的数据集开始。这个数据集分为三个部分:</p><ul><li>用于模型学习的训练集:x, y</li><li>交叉验证集，用于确定正则化参数:xval, yval</li><li>测试集，用于评估性能。这些是您的模型在培训期间没有看到的“不可见”示例:Xtest, ytest</li></ul><h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line"></span><br><span class="line">path = <span class="string">'ex5data1.mat'</span></span><br><span class="line">data = loadmat(path)</span><br><span class="line"><span class="comment">#Training set</span></span><br><span class="line">X, y = data[<span class="string">'X'</span>], data[<span class="string">'y'</span>]</span><br><span class="line"><span class="comment">#Cross validation set</span></span><br><span class="line">Xval, yval = data[<span class="string">'Xval'</span>], data[<span class="string">'yval'</span>]</span><br><span class="line"><span class="comment">#Test set</span></span><br><span class="line">Xtest, ytest = data[<span class="string">'Xtest'</span>], data[<span class="string">'ytest'</span>]</span><br><span class="line"><span class="comment">#Insert a column of 1's to all of the X's, as usual</span></span><br><span class="line">X = np.insert(X    ,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</span><br><span class="line">Xval = np.insert(Xval ,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</span><br><span class="line">Xtest = np.insert(Xtest,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'X=&#123;&#125;,y=&#123;&#125;'</span>.format(X.shape, y.shape))</span><br><span class="line">print(<span class="string">'Xval=&#123;&#125;,yval=&#123;&#125;'</span>.format(Xval.shape, yval.shape))</span><br><span class="line">print(<span class="string">'Xtest=&#123;&#125;,ytest=&#123;&#125;'</span>.format(Xtest.shape, ytest.shape))</span><br></pre></td></tr></table></figure><pre><code>X=(12, 2),y=(12, 1)Xval=(21, 2),yval=(21, 1)Xtest=(21, 2),ytest=(21, 1)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotData</span><span class="params">()</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">    plt.scatter(X[:,<span class="number">1</span>:],y,c=<span class="string">'r'</span>,marker=<span class="string">'x'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Change in water level(x)'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Water flowing out of the dam(y)'</span>)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">plotData()</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_1.png" alt="png"></p><h2 id="正则化线性回归代价函数"><a href="#正则化线性回归代价函数" class="headerlink" title="正则化线性回归代价函数"></a>正则化线性回归代价函数</h2><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}^2+\frac{\lambda}{2m}\sum_{j=1}^n{\theta_j}^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costReg</span><span class="params">(theta, X, y, l)</span>:</span></span><br><span class="line">    <span class="string">'''do not regularizethe theta0</span></span><br><span class="line"><span class="string">    theta is a 1-d array with shape (n+1,)</span></span><br><span class="line"><span class="string">    X is a matrix with shape (m, n+1)</span></span><br><span class="line"><span class="string">    y is a matrix with shape (m, 1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    cost = ((X @ theta - y.flatten()) ** <span class="number">2</span>).sum()</span><br><span class="line">    regterm = l * (theta[<span class="number">1</span>:] @ theta[<span class="number">1</span>:])</span><br><span class="line">    <span class="keyword">return</span> (cost + regterm) / (<span class="number">2</span> * len(X))</span><br><span class="line"></span><br><span class="line">theta = np.ones(X.shape[<span class="number">1</span>])</span><br><span class="line">print(X.shape,theta.shape,(X@theta).shape,(y.flatten()).shape,(X @ theta - y.flatten()).shape)</span><br><span class="line">print(costReg(theta,X,y,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>(12, 2) (2,) (12,) (12,) (12,)303.9931922202643</code></pre><h2 id="正则化线性回归梯度"><a href="#正则化线性回归梯度" class="headerlink" title="正则化线性回归梯度"></a>正则化线性回归梯度</h2><ul><li>for j = 0<script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial\theta_0}=\frac{1}{m}\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}{x_j}^{(i)}</script></li><li>for j ≥ 0<script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial\theta_j}=\frac{1}{m}\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}{x_j}^{(i)}+\frac{\lambda}{m}\theta_j</script></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientReg</span><span class="params">(theta, X, y, l)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    theta: 1-d array with shape (2,)</span></span><br><span class="line"><span class="string">    X: 2-d array with shape (12, 2)</span></span><br><span class="line"><span class="string">    y: 2-d array with shape (12, 1)</span></span><br><span class="line"><span class="string">    l: lambda constant</span></span><br><span class="line"><span class="string">    grad has same shape as theta (2,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grad = (X @ theta - y.flatten()) @ X</span><br><span class="line">    regterm = l * theta</span><br><span class="line">    regterm[<span class="number">0</span>] = <span class="number">0</span>  <span class="comment"># #don't regulate bias term</span></span><br><span class="line">    <span class="keyword">return</span> (grad + regterm) / len(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using theta initialized at [1; 1] you should expect to see a </span></span><br><span class="line"><span class="comment"># gradient of [-15.303016; 598.250744] (with lambda=1)</span></span><br><span class="line">print(gradientReg(theta, X, y, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>(2,)[-15.30301567 598.25074417]</code></pre><h2 id="拟合线性回归"><a href="#拟合线性回归" class="headerlink" title="拟合线性回归"></a>拟合线性回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainLinearReg</span><span class="params">(X, y, l)</span>:</span></span><br><span class="line">    theta = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">    res = opt.minimize(fun=costReg, </span><br><span class="line">                       x0=theta, </span><br><span class="line">                       args=(X, y ,l), </span><br><span class="line">                       method=<span class="string">'TNC'</span>, </span><br><span class="line">                       jac=gradientReg)</span><br><span class="line">    <span class="keyword">return</span> res.x</span><br><span class="line"></span><br><span class="line">fit_theta = trainLinearReg(X, y, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plotData()</span><br><span class="line">plt.plot(X[:,<span class="number">1</span>], X @ fit_theta)</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_2.png" alt="png"></p><p>这里我们把$\lambda$ = 0，因为我们现在实现的线性回归只有两个参数，这么低的维度，正则化并没有用。从图中可以看到，拟合最好的这条直线告诉我们这个模型并不适合这个数据。<br>在下一节中，您将实现一个函数来生成学习曲线，它可以帮助您调试学习算法，即使可视化数据不那么容易。</p><h1 id="偏差和误差"><a href="#偏差和误差" class="headerlink" title="偏差和误差"></a>偏差和误差</h1><p>机器学习中一个重要的概念是偏差（bias）和方差（variance）的权衡。高偏差意味着欠拟合，高方差意味着过拟合。<br>在这部分练习中，您将在学习曲线上绘制训练误差和验证误差，以诊断bias-variance问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(X, y, Xval, yval, l)</span>:</span></span><br><span class="line">    <span class="string">"""画出学习曲线，即交叉验证误差和训练误差随样本数量的变化的变化"""</span></span><br><span class="line">    xx = range(<span class="number">1</span>, len(X) + <span class="number">1</span>)  <span class="comment"># at least has one example </span></span><br><span class="line">    training_cost, cv_cost = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xx:</span><br><span class="line">        res = trainLinearReg(X[:i], y[:i], l)</span><br><span class="line">        training_cost_i = costReg(res, X[:i], y[:i], <span class="number">0</span>)</span><br><span class="line">        cv_cost_i = costReg(res, Xval, yval, <span class="number">0</span>)</span><br><span class="line">        training_cost.append(training_cost_i)</span><br><span class="line">        cv_cost.append(cv_cost_i)</span><br><span class="line">        </span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">    plt.plot(xx, training_cost, label=<span class="string">'training cost'</span>)  </span><br><span class="line">    plt.plot(xx, cv_cost, label=<span class="string">'cv cost'</span>) </span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.xlabel(<span class="string">'Number of training examples'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Error'</span>)</span><br><span class="line">    plt.title(<span class="string">'Learning curve for linear regression'</span>)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plot_learning_curve(X,y,Xval,yval,<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_3.png" alt="png"></p><p>从图中看出来，随着样本数量的增加，训练误差和交叉验证误差都很高，这属于高偏差，欠拟合。</p><h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>我们的线性模型对于数据来说太简单了，导致了欠拟合(高偏差)。在这一部分的练习中，您将通过添加更多的特性来解决这个问题。</p><h3 id="Learning-Polynomial-Regression"><a href="#Learning-Polynomial-Regression" class="headerlink" title="Learning Polynomial Regression"></a>Learning Polynomial Regression</h3><p>数据预处理</p><ul><li>X，Xval，Xtest都需要添加多项式特征，这里我们选择增加到6次方，因为若选8次方无法达到作业pdf上的效果图，这是因为scipy和octave版本的优化算法不同。</li><li>不要忘了标准化。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genPolyFeatures</span><span class="params">(X, power)</span>:</span></span><br><span class="line">    <span class="string">"""添加多项式特征</span></span><br><span class="line"><span class="string">    每次在array的最后一列插入第二列的i+2次方（第一列为偏置）</span></span><br><span class="line"><span class="string">    从二次方开始开始插入（因为本身含有一列一次方）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    Xpoly = X.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, power + <span class="number">1</span>):</span><br><span class="line">        Xpoly = np.insert(Xpoly, Xpoly.shape[<span class="number">1</span>], np.power(Xpoly[:,<span class="number">1</span>], i), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Xpoly</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_means_std</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">"""获取训练集的均值和误差，用来标准化所有数据。"""</span></span><br><span class="line">    means = np.mean(X,axis=<span class="number">0</span>)</span><br><span class="line">    stds = np.std(X,axis=<span class="number">0</span>,ddof=<span class="number">1</span>)  <span class="comment"># ddof=1 means 样本标准差</span></span><br><span class="line">    <span class="keyword">return</span> means, stds</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">featureNormalize</span><span class="params">(myX, means, stds)</span>:</span></span><br><span class="line">    <span class="string">"""标准化"""</span></span><br><span class="line">    X_norm = myX.copy()</span><br><span class="line">    X_norm[:,<span class="number">1</span>:] = X_norm[:,<span class="number">1</span>:] - means[<span class="number">1</span>:]</span><br><span class="line">    X_norm[:,<span class="number">1</span>:] = X_norm[:,<span class="number">1</span>:] / stds[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> X_norm</span><br></pre></td></tr></table></figure><p>关于归一化，所有数据集应该都用<strong>训练集的均值和样本标准差</strong>处理。切记。所以要将训练集的均值和样本标准差存储起来，对后面的数据进行处理。</p><p>而且注意这里是样本标准差而不是总体标准差，使用np.std()时，将ddof=1则是样本标准差，默认=0是总体标准差。而pandas默认计算样本标准差。</p><p>获取添加多项式特征以及标准化之后的数据.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">power = <span class="number">6</span>  <span class="comment"># 扩展到x的6次方</span></span><br><span class="line"></span><br><span class="line">train_means, train_stds = get_means_std(genPolyFeatures(X,power))</span><br><span class="line">X_norm = featureNormalize(genPolyFeatures(X,power), train_means, train_stds)</span><br><span class="line">Xval_norm = featureNormalize(genPolyFeatures(Xval,power), train_means, train_stds)</span><br><span class="line">Xtest_norm = featureNormalize(genPolyFeatures(Xtest,power), train_means, train_stds)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_fit</span><span class="params">(means, stds, l)</span>:</span></span><br><span class="line">    <span class="string">"""画出拟合曲线"""</span></span><br><span class="line">    theta = trainLinearReg(X_norm,y, l)</span><br><span class="line">    x = np.linspace(<span class="number">-75</span>,<span class="number">55</span>,<span class="number">50</span>)</span><br><span class="line">    xmat = x.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    xmat = np.insert(xmat,<span class="number">0</span>,<span class="number">1</span>,axis=<span class="number">1</span>)</span><br><span class="line">    Xmat = genPolyFeatures(xmat, power)</span><br><span class="line">    Xmat_norm = featureNormalize(Xmat, means, stds)</span><br><span class="line">    plotData()</span><br><span class="line">    plt.plot(x, Xmat_norm@theta,<span class="string">'b--'</span>)</span><br><span class="line"></span><br><span class="line">plot_fit(train_means, train_stds, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_4.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(X_norm, y, Xval_norm, yval, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_5.png" alt="png"></p><h3 id="调整正则化参数-lambda"><a href="#调整正则化参数-lambda" class="headerlink" title="调整正则化参数$\lambda$"></a>调整正则化参数$\lambda$</h3><p>上图可以看到$\lambda=0$时，训练误差太小了，明显过拟合了。<br>我们继续调整$\lambda=1$时:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_fit(train_means, train_stds, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_6.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(X_norm, y, Xval_norm, yval, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_7.png" alt="png"></p><p>我们继续调整λ \lambdaλ = 100 时，很明显惩罚过多，欠拟合了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_fit(train_means, train_stds, <span class="number">100</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_8.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(X_norm, y, Xval_norm, yval, <span class="number">100</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_9.png" alt="png"></p><h3 id="Selecting-λ-using-a-cross-validation-set"><a href="#Selecting-λ-using-a-cross-validation-set" class="headerlink" title="Selecting λ using a cross validation set"></a>Selecting λ using a cross validation set</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">lambdas = [<span class="number">0.</span>, <span class="number">0.001</span>, <span class="number">0.003</span>, <span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1.</span>, <span class="number">3.</span>, <span class="number">10.</span>]</span><br><span class="line">errors_train, errors_val = [], []</span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> lambdas:</span><br><span class="line">    theta = trainLinearReg(X_norm, y, l)</span><br><span class="line">    errors_train.append(costReg(theta,X_norm,y,<span class="number">0</span>))  <span class="comment"># 记得把lambda = 0</span></span><br><span class="line">    errors_val.append(costReg(theta,Xval_norm,yval,<span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(lambdas,errors_train,label=<span class="string">'Train'</span>)</span><br><span class="line">plt.plot(lambdas,errors_val,label=<span class="string">'Cross Validation'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'lambda'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Error'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/9.9_10.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以看到时交叉验证代价最小的是 lambda = 3</span></span><br><span class="line">print(lambdas[np.argmin(errors_val)]) <span class="comment"># 3.0</span></span><br></pre></td></tr></table></figure><pre><code>3.0</code></pre><h3 id="Computing-test-set-error"><a href="#Computing-test-set-error" class="headerlink" title="Computing test set error"></a>Computing test set error</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">theta = trainLinearReg(X_norm, y, <span class="number">3</span>)</span><br><span class="line">print(<span class="string">'test cost(l=&#123;&#125;) = &#123;&#125;'</span>.format(<span class="number">3</span>, costReg(theta, Xtest_norm, ytest, <span class="number">0</span>)))</span><br><span class="line"><span class="comment"># for l in lambdas:</span></span><br><span class="line"><span class="comment">#     theta = trainLinearReg(X_norm, y, l)</span></span><br><span class="line"><span class="comment">#     print('test cost(l=&#123;&#125;) = &#123;&#125;'.format(l, costReg(theta, Xtest_norm, ytest, 0)))</span></span><br></pre></td></tr></table></figure><pre><code>test cost(l=3) = 4.755271964962211</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在训练的前半部分，你将利用水库水位的变化，实现对大坝泄洪量的正则线性回归预测。在接下来的半部分中，你将通过一些来诊断法调试学习算法，并检查偏差和方差的影响。&lt;/p&gt;
&lt;h1 id=&quot;正则线性回归&quot;&gt;&lt;a href=&quot;#正则线性回归&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
      <category term="python" scheme="https://hiworldalice.github.io/categories/python/"/>
    
    
      <category term="机器学习" scheme="https://hiworldalice.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达机器学习作业Python实现(四)：神经网络（反向传播）</title>
    <link href="https://hiworldalice.github.io/2019/08/28/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%89/"/>
    <id>https://hiworldalice.github.io/2019/08/28/神经网络（反向传播）/</id>
    <published>2019-08-28T07:31:30.000Z</published>
    <updated>2019-08-28T07:37:03.336Z</updated>
    
    <content type="html"><![CDATA[<p>在这个练习中，你将实现反向传播算法来学习神经网络的参数。依旧是上次预测手写数数字的例子。</p><h1 id="1-Neural-Networks"><a href="#1-Neural-Networks" class="headerlink" title="1.Neural Networks"></a>1.Neural Networks</h1><h2 id="Visualizing-the-data"><a href="#Visualizing-the-data" class="headerlink" title="Visualizing the data"></a>Visualizing the data</h2><p>这部分我们随机选取100个样本并可视化。训练集共有5000个训练样本，每个样本是20×20像素的数字的灰度图像。每个像素代表一个浮点数，表示该位置的灰度强度。20×20的像素网格被展开成一个400维的向量。在我们的数据矩阵X中，每一个样本都变成了一行，这给了我们一个5000×400矩阵X，每一行都是一个手写数字图像的训练样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report  <span class="comment">#这个包是评价报告</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_path</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="string">'''读取数据'''</span></span><br><span class="line">    data = loadmat(path)  <span class="comment">#return a dict</span></span><br><span class="line">    X = data[<span class="string">'X'</span>]</span><br><span class="line">    y = data[<span class="string">'y'</span>].flatten()</span><br><span class="line">    <span class="keyword">return</span> X,y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_100_images</span><span class="params">(X)</span>:</span></span><br><span class="line">    index = np.random.choice(range(<span class="number">5000</span>),<span class="number">100</span>)   </span><br><span class="line">    images = X[index,:]</span><br><span class="line">    <span class="comment">#plt.subplots()返回值ax为object or array of Axes objects</span></span><br><span class="line">    fig,ax_array = plt.subplots(nrows=<span class="number">10</span>, ncols=<span class="number">10</span>, sharey=<span class="literal">True</span>, sharex=<span class="literal">True</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> column <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            ax_array[row, column].matshow(images[<span class="number">10</span> * row + column].reshape((<span class="number">20</span>, <span class="number">20</span>)),</span><br><span class="line">                                   cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])        </span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">raw_X,raw_y = load_path(<span class="string">'ex4data1.mat'</span>)</span><br><span class="line">plot_100_images(raw_X)</span><br></pre></td></tr></table></figure><p><img src="/images/8.28_1.png" alt="png"></p><h2 id="Model-representation"><a href="#Model-representation" class="headerlink" title="Model representation"></a>Model representation</h2><p>我们的网络有三层，输入层，隐藏层，输出层。我们的输入是数字图像的像素值，因为每个数字的图像大小为20X20，所以我们输入层有400个单元（这里不包括总是输出要加一个偏置单元）。</p><p><img src="/images/8.28_2.jpg" alt="jpg"></p><h3 id="Load-train-data-set"><a href="#Load-train-data-set" class="headerlink" title="Load train data set"></a>Load train data set</h3><p>首先我们要将标签值（1，2，3，4，…，10）转化成非线性相关的向量，向量对应位置（y_array[i-1]）上的值等于1，例如y[0]=6转化为y[0]=[0,0,0,0,0,1,0,0,0,0]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_y</span><span class="params">(y)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="comment"># 把y中每个类别转化为一个向量，对应的lable值在向量对应位置上置为1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> y:</span><br><span class="line">        y_array = np.zeros(<span class="number">10</span>)</span><br><span class="line">        y_array[i<span class="number">-1</span>] = <span class="number">1</span></span><br><span class="line">        result.append(y_array)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    # 或者用sklearn中OneHotEncoder函数</span></span><br><span class="line"><span class="string">    encoder =  OneHotEncoder(sparse=False)  # return a array instead of matrix</span></span><br><span class="line"><span class="string">    y_onehot = encoder.fit_transform(y.reshape(-1,1))</span></span><br><span class="line"><span class="string">    return y_onehot</span></span><br><span class="line"><span class="string">    '''</span> </span><br><span class="line">    <span class="comment">#将列表转为数组返回</span></span><br><span class="line">    <span class="keyword">return</span> np.array(result)</span><br></pre></td></tr></table></figure><p>对训练集做相应的处理，得到我们的input X，lables y.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = np.insert(raw_X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = expand_y(raw_y)</span><br><span class="line">print(X.shape)</span><br><span class="line">print(y.shape)</span><br></pre></td></tr></table></figure><pre><code>(5000, 401)(5000, 10)</code></pre><h3 id="Load-weight"><a href="#Load-weight" class="headerlink" title="Load weight"></a>Load weight</h3><p>这里我们提供了已经训练好的参数$\theta_1$、$\theta_2$，存储在ex4weight.mat文件中。这些参数的维度由神经网络的大小决定，第二层有25个单元，输出层有10个单元(对应10个数字类)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = loadmat(path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'Theta1'</span>], data[<span class="string">'Theta2'</span>] </span><br><span class="line"></span><br><span class="line">t1, t2 = load_weight(<span class="string">'ex4weights.mat'</span>)</span><br><span class="line">print(t1.shape, t2.shape)</span><br></pre></td></tr></table></figure><pre><code>(25, 401) (10, 26)</code></pre><p><strong>当我们使用高级优化方法来优化神经网络时，我们需要将多个参数矩阵展开，才能传入优化函数，然后再恢复形状。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serialize</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="string">'''展开参数'''</span></span><br><span class="line">    <span class="keyword">return</span> np.r_[a.flatten(),b.flatten()]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deserialize</span><span class="params">(seq)</span>:</span></span><br><span class="line">    <span class="string">'''提取参数'''</span></span><br><span class="line">    <span class="keyword">return</span> seq[:<span class="number">25</span>*<span class="number">401</span>].reshape(<span class="number">25</span>, <span class="number">401</span>), seq[<span class="number">25</span>*<span class="number">401</span>:].reshape(<span class="number">10</span>, <span class="number">26</span>)</span><br><span class="line"></span><br><span class="line">theta = serialize(t1, t2)  <span class="comment"># 扁平化参数，25*401+10*26=10285</span></span><br><span class="line">print(theta.shape)</span><br></pre></td></tr></table></figure><pre><code>(10285,)</code></pre><h2 id="Feedforward-and-cost-function"><a href="#Feedforward-and-cost-function" class="headerlink" title="Feedforward and cost function"></a>Feedforward and cost function</h2><h3 id="Feedforward"><a href="#Feedforward" class="headerlink" title="Feedforward"></a>Feedforward</h3><p>确保每层的单元数，注意输出时加一个偏置单元，s(1)=400+1，s(2)=25+1，s(3)=10。</p><p><img src="/images/8.28_2.jpg" alt="jpg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    <span class="string">'''得到每层的输入和输出'''</span></span><br><span class="line">    t1, t2 = deserialize(theta)</span><br><span class="line">    <span class="comment"># 前面已经插入过偏置单元，这里就不用插入了</span></span><br><span class="line">    a1 = X</span><br><span class="line">    z2 = a1 @ t1.T</span><br><span class="line">    a2 = np.insert(sigmoid(z2), <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">    z3 = a2 @ t2.T</span><br><span class="line">    a3 = sigmoid(z3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a1, z2, a2, z3, a3 </span><br><span class="line"></span><br><span class="line">a1, z2, a2, z3, h = feed_forward(theta, X)</span><br></pre></td></tr></table></figure><h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p>神经网络的代价函数（不带正则化项）</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K{[y_k^{(i)}log((h_\theta(x^{(i)}))_k)+{(1-y_k^{(i)})}{log(1-(h_\theta(x^{(i)}))_k)]}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    a1, z2, a2, z3, h = feed_forward(theta, X)</span><br><span class="line">    J = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        first = - y[i] * np.log(h[i])</span><br><span class="line">        second = (<span class="number">1</span> - y[i]) * np.log(<span class="number">1</span> - h[i])</span><br><span class="line">        J = J + np.sum(first - second)</span><br><span class="line">    J = J / len(X)</span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">     # or just use verctorization</span></span><br><span class="line"><span class="string">     J = - y * np.log(h) - (1 - y) * np.log(1 - h)</span></span><br><span class="line"><span class="string">     return J.sum() / len(X)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">cost(theta, X, y)</span><br></pre></td></tr></table></figure><pre><code>0.2876291651613188</code></pre><h2 id="Regularized-cost-function"><a href="#Regularized-cost-function" class="headerlink" title="Regularized cost function"></a>Regularized cost function</h2><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K{[y_k^{(i)}log((h_\theta(x^{(i)}))_k)+{(1-y_k^{(i)})}{log(1-(h_\theta(x^{(i)}))_k)]}}+\frac{\lambda}{2m}[\sum_{j=1}^{25}\sum_{k=1}^{400}{(\theta_{jk}^{(1)})}^2+\sum_{j=1}^{10}\sum_{k=1}^{25}{(\theta_{jk}^{(2)})}^2]</script><p>**注意不要将每层的偏置项正则化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">'''正则化时忽略每层的偏置项，也就是参数矩阵的第一列'''</span></span><br><span class="line">    t1, t2 = deserialize(theta)</span><br><span class="line">    reg = np.sum(t1[:,<span class="number">1</span>:] ** <span class="number">2</span>) + np.sum(t2[:,<span class="number">1</span>:] ** <span class="number">2</span>)  <span class="comment"># or use np.power(a, 2)</span></span><br><span class="line">    <span class="keyword">return</span> l / (<span class="number">2</span> * len(X)) * reg + cost(theta, X, y)</span><br><span class="line"></span><br><span class="line">print(regularized_cost(theta,X,y,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>0.38376985909092354</code></pre><h1 id="2-Backpropagation"><a href="#2-Backpropagation" class="headerlink" title="2.Backpropagation"></a>2.Backpropagation</h1><h2 id="Sigmoid-gradient-S型函数导数"><a href="#Sigmoid-gradient-S型函数导数" class="headerlink" title="Sigmoid gradient S型函数导数"></a>Sigmoid gradient S型函数导数</h2><script type="math/tex; mode=display">g^`(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z) * (<span class="number">1</span> - sigmoid(z))</span><br></pre></td></tr></table></figure><h2 id="Random-initialization"><a href="#Random-initialization" class="headerlink" title="Random initialization"></a>Random initialization</h2><p>当我们训练神经网络时，随机初始化参数是很重要的，可以打破数据的对称性。一个有效的策略是在均匀分布(−e，e)中随机选择值，我们可以选择 e = 0.12 这个范围的值来确保参数足够小，使得训练更有效率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_init</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">'''从服从的均匀分布的范围中随机返回size大小的值'''</span></span><br><span class="line">    <span class="keyword">return</span> np.random.uniform(<span class="number">-0.12</span>, <span class="number">0.12</span>, size)</span><br></pre></td></tr></table></figure><h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p><img src="/images/8.28_3.jpg" alt="jpg"></p><p>目标：获取整个网络代价函数的梯度。以便在优化算法中求解。</p><p>这里面一定要理解正向传播和反向传播的过程，才能弄清楚各种参数在网络中的维度，切记。比如手写出每次传播的式子。</p><p><img src="/images/8.28_4.PNG" alt="PNG"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'a1'</span>, a1.shape,<span class="string">'t1'</span>, t1.shape)</span><br><span class="line">print(<span class="string">'z2'</span>, z2.shape)</span><br><span class="line">print(<span class="string">'a2'</span>, a2.shape, <span class="string">'t2'</span>, t2.shape)</span><br><span class="line">print(<span class="string">'z3'</span>, z3.shape)</span><br><span class="line">print(<span class="string">'a3'</span>, h.shape)</span><br></pre></td></tr></table></figure><pre><code>a1 (5000, 401) t1 (25, 401)z2 (5000, 25)a2 (5000, 26) t2 (10, 26)z3 (5000, 10)a3 (5000, 10)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    unregularized gradient, notice no d1 since the input layer has no error </span></span><br><span class="line"><span class="string">    return 所有参数theta的梯度，故梯度D(i)和参数theta(i)同shape，重要。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    t1, t2 = deserialize(theta)</span><br><span class="line">    a1, z2, a2, z3, h = feed_forward(theta, X)</span><br><span class="line">    d3 = h - y <span class="comment"># (5000, 10)</span></span><br><span class="line">    d2 = d3 @ t2[:,<span class="number">1</span>:] * sigmoid_gradient(z2)  <span class="comment"># (5000, 25)</span></span><br><span class="line">    D2 = d3.T @ a2  <span class="comment"># (10, 26)</span></span><br><span class="line">    D1 = d2.T @ a1 <span class="comment"># (25, 401)</span></span><br><span class="line">    D = (<span class="number">1</span> / len(X)) * serialize(D1, D2)  <span class="comment"># (10285,)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> D</span><br></pre></td></tr></table></figure><h2 id="Gradient-checking"><a href="#Gradient-checking" class="headerlink" title="Gradient checking"></a>Gradient checking</h2><p>通过$\theta_i-J(\theta_i)$图中切线的斜率，针对各$\theta$都计算出一个实际的近似梯度值，将这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同D进行比较。np.linalg.norm(求范数)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_checking</span><span class="params">(theta, X, y, e)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a_numeric_grad</span><span class="params">(plus, minus)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        对每个参数theta_i计算数值梯度，即理论梯度。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (e * <span class="number">2</span>)</span><br><span class="line">   </span><br><span class="line">    numeric_grad = [] </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta)):</span><br><span class="line">        plus = theta.copy()  <span class="comment"># deep copy otherwise you will change the raw theta</span></span><br><span class="line">        minus = theta.copy()</span><br><span class="line">        plus[i] = plus[i] + e</span><br><span class="line">        minus[i] = minus[i] - e</span><br><span class="line">        grad_i = a_numeric_grad(plus, minus)</span><br><span class="line">        numeric_grad.append(grad_i)</span><br><span class="line">    </span><br><span class="line">    numeric_grad = np.array(numeric_grad)</span><br><span class="line">    analytic_grad = regularized_gradient(theta, X, y)</span><br><span class="line">    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: &#123;&#125;\n'</span>.format(diff))</span><br></pre></td></tr></table></figure><h2 id="Regularized-Neural-Networks正则化神经网络"><a href="#Regularized-Neural-Networks正则化神经网络" class="headerlink" title="Regularized Neural Networks正则化神经网络"></a>Regularized Neural Networks正则化神经网络</h2><script type="math/tex; mode=display">\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)=D_{ij}^{(l)}=\frac{1}{m}\nabla_{ij}^{(l)}     \quad \quad \quad     for\ j=0</script><script type="math/tex; mode=display">\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)=D_{ij}^{(l)}=\frac{1}{m}\nabla_{ij}^{(l)}+\frac{\lambda}{m}\theta_{ij}^{(l)}     \quad \quad      for\ j≥0</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""不惩罚偏置单元的参数"""</span></span><br><span class="line">    a1, z2, a2, z3, h = feed_forward(theta, X)</span><br><span class="line">    D1, D2 = deserialize(gradient(theta, X, y))</span><br><span class="line">    t1[:,<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    t2[:,<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    reg_D1 = D1 + (l / len(X)) * t1</span><br><span class="line">    reg_D2 = D2 + (l / len(X)) * t2</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> serialize(reg_D1, reg_D2)</span><br></pre></td></tr></table></figure><h2 id="Learning-parameters"><a href="#Learning-parameters" class="headerlink" title="Learning parameters"></a>Learning parameters</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_training</span><span class="params">(X, y)</span>:</span></span><br><span class="line">    init_theta = random_init(<span class="number">10285</span>)  <span class="comment"># 25*401 + 10*26</span></span><br><span class="line"></span><br><span class="line">    res = opt.minimize(fun=regularized_cost,</span><br><span class="line">                       x0=init_theta,</span><br><span class="line">                       args=(X, y, <span class="number">1</span>),</span><br><span class="line">                       method=<span class="string">'TNC'</span>,</span><br><span class="line">                       jac=regularized_gradient,</span><br><span class="line">                       options=&#123;<span class="string">'maxiter'</span>: <span class="number">400</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">res = nn_training(X,y)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><pre><code>     fun: 0.5747214559102154     jac: array([ 1.01374918e-03, -2.11248326e-12,  4.38829369e-13, ...,        1.09523957e-03,  1.33366590e-03,  1.90893576e-03]) message: &#39;Converged (|f_n-f_(n-1)| ~= 0)&#39;    nfev: 207     nit: 14  status: 1 success: True       x: array([ 0.92587959, -0.01586693,  0.06154528, ..., -5.34601809,       -3.11598214,  1.56864239])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    _, _, _, _, h = feed_forward(res.x, X)</span><br><span class="line">    y_pred = np.argmax(h, axis=<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    print(classification_report(y, y_pred))</span><br><span class="line"></span><br><span class="line">print(accuracy(res.x,X,raw_y))</span><br></pre></td></tr></table></figure><pre><code>             precision    recall  f1-score   support          1       0.98      0.99      0.98       500          2       0.96      0.95      0.96       500          3       0.98      0.95      0.96       500          4       0.96      0.97      0.97       500          5       0.96      0.96      0.96       500          6       0.98      0.99      0.98       500          7       0.97      0.96      0.97       500          8       0.95      0.96      0.96       500          9       0.97      0.95      0.96       500         10       0.97      1.00      0.98       500avg / total       0.97      0.97      0.97      5000None</code></pre><h1 id="Visualizing-the-hidden-layer"><a href="#Visualizing-the-hidden-layer" class="headerlink" title="Visualizing the hidden layer"></a>Visualizing the hidden layer</h1><p>理解神经网络是如何学习的一个很好的办法是，可视化隐藏层单元所捕获的内容。通俗的说，给定一个的隐藏层单元，可视化它所计算的内容的方法是找到一个输入x，x可以激活这个单元（也就是说有一个激活值$a_i^{(l)}$接近与1）。对于我们所训练的网络，注意到θ1中每一行都是一个401维的向量，代表每个隐藏层单元的参数。如果我们忽略偏置项，我们就能得到400维的向量，这个向量代表每个样本输入到每个隐层单元的像素的权重。因此可视化的一个方法是，reshape这个400维的向量为（20，20）的图像然后输出。<br>在我们经过训练的网络中，你应该发现隐藏的单元大致对应于在输入中寻找笔画和其他模式的检测器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_hidden</span><span class="params">(theta)</span>:</span></span><br><span class="line">    t1, _ = deserialize(theta)</span><br><span class="line">    t1 = t1[:, <span class="number">1</span>:]</span><br><span class="line">    fig,ax_array = plt.subplots(<span class="number">5</span>, <span class="number">5</span>, sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>, figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            ax_array[r, c].matshow(t1[r * <span class="number">5</span> + c].reshape(<span class="number">20</span>, <span class="number">20</span>), cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">            plt.xticks([])</span><br><span class="line">            plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_hidden(res.x)</span><br></pre></td></tr></table></figure><p><img src="/images/8.28_5.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在这个练习中，你将实现反向传播算法来学习神经网络的参数。依旧是上次预测手写数数字的例子。&lt;/p&gt;
&lt;h1 id=&quot;1-Neural-Networks&quot;&gt;&lt;a href=&quot;#1-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;1.Neur
      
    
    </summary>
    
      <category term="python" scheme="https://hiworldalice.github.io/categories/python/"/>
    
    
      <category term="机器学习" scheme="https://hiworldalice.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Python多线程</title>
    <link href="https://hiworldalice.github.io/2019/08/12/Python%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    <id>https://hiworldalice.github.io/2019/08/12/Python多线程/</id>
    <published>2019-08-12T08:10:34.000Z</published>
    <updated>2019-08-12T08:17:43.967Z</updated>
    
    <content type="html"><![CDATA[<p>多线程类似于同时执行多个不同程序，多线程运行有如下优点：</p><ul><li>使用线程可以把占据长时间的程序中的任务放到后台去处理。</li><li>程序的运行速度可能加快。</li><li>对于I/O密集型Python程序如用户输入、文件读写和网络收发数据等，线程就比较有用了。在这种情况下我们可以释放一些珍贵的资源如内存占用等等。</li></ul><p>每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。<br>每个线程都有他自己的一组CPU寄存器，称为线程的上下文，该上下文反映了线程上次运行该线程的CPU寄存器的状态。<br>指令指针和堆栈指针寄存器是线程上下文中两个最重要的寄存器，线程总是在进程得到上下文中运行的，这些地址都用于标志拥有线程的进程地址空间中的内存。</p><ul><li>线程可以被抢占（中断）。</li><li>在其他线程正在运行时，线程可以暂时搁置（也称为睡眠） — 这就是线程的退让。</li><li>线程无法给予公平的执行时间。</li></ul><p>Python3 线程中常用的两个模块为：</p><ul><li>_thread</li><li>threading(推荐使用)</li></ul><p>thread 模块已被废弃。用户可以使用 threading 模块代替。所以，在 Python3 中不能再使用”thread” 模块。为了兼容性，Python3 将 thread 重命名为 “_thread”。</p><h1 id="Python中使用多线程"><a href="#Python中使用多线程" class="headerlink" title="Python中使用多线程"></a>Python中使用多线程</h1><p>Python中使用线程有两种方式：函数或者用类来包装线程对象。</p><h2 id="函数start-new-thread"><a href="#函数start-new-thread" class="headerlink" title="函数start_new_thread()"></a>函数start_new_thread()</h2><p>调用_thread模块中的start_new_thread()函数来产生新线程、<br><strong>_thread.start_new_thread ( function, args, kwargs=None] )</strong><br>参数说明：</p><ul><li>function 线程函数</li><li>args 传递给线程函数的参数,必须是个tuple类型。即使function不需要参        数，也需要传递一个空数组</li><li>kwargs 可选参数</li></ul><p>实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> _thread</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为线程定义一个函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_time</span><span class="params">( threadName, delay)</span>:</span></span><br><span class="line">   count = <span class="number">0</span></span><br><span class="line">   <span class="keyword">while</span> count &lt; <span class="number">5</span>:</span><br><span class="line">      time.sleep(delay)</span><br><span class="line">      count += <span class="number">1</span></span><br><span class="line">      <span class="keyword">print</span> (<span class="string">"%s: %s"</span> % ( threadName, time.ctime(time.time()) ))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建两个线程</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">   _thread.start_new_thread( print_time, (<span class="string">"Thread-1"</span>, <span class="number">2</span>, ) )</span><br><span class="line">   _thread.start_new_thread( print_time, (<span class="string">"Thread-2"</span>, <span class="number">4</span>, ) )</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">   <span class="keyword">print</span> (<span class="string">"Error: 无法启动线程"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">   <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Thread-1: Mon Aug 12 16:03:32 2019</span><br><span class="line">Thread-2: Mon Aug 12 16:03:34 2019</span><br><span class="line">Thread-1: Mon Aug 12 16:03:35 2019</span><br><span class="line">Thread-1: Mon Aug 12 16:03:37 2019</span><br><span class="line">Thread-2: Mon Aug 12 16:03:38 2019</span><br><span class="line">Thread-1: Mon Aug 12 16:03:39 2019</span><br><span class="line">Thread-1: Mon Aug 12 16:03:41 2019</span><br><span class="line">Thread-2: Mon Aug 12 16:03:43 2019</span><br><span class="line">Thread-2: Mon Aug 12 16:03:47 2019</span><br><span class="line">Thread-2: Mon Aug 12 16:03:51 2019</span><br></pre></td></tr></table></figure><h2 id="threading模块"><a href="#threading模块" class="headerlink" title="threading模块"></a>threading模块</h2><p>Python3 通过两个标准库 _thread 和 threading 提供对线程的支持。<br>_thread 提供了低级别的、原始的线程以及一个简单的锁，它相比于 threading 模块的功能还是比较有限的。</p><p>threading 模块除了包含 _thread 模块中的所有方法外，还提供的其他方法：</p><ul><li>threading.currentThread(): 返回当前的线程变量。</li><li>threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。</li><li>threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果。</li></ul><p>除了使用方法外，线程模块同样提供了<strong>Thread类</strong>来处理线程，Thread类提供了以下方法:</p><ul><li>run(): 用以表示线程活动的方法。</li><li>start():启动线程活动。</li><li>join([time]): 等待至线程中止。这阻塞调用线程直至线程的join() 方法被调用中止-正常退出或者抛出未处理的异常-或者是可选的超时发生。</li><li>isAlive(): 返回线程是否活动的。</li><li>getName(): 返回线程名。</li><li>setName(): 设置线程名。</li></ul><p>我们可以通过直接从 threading.Thread 继承创建一个新的子类，并实例化后调用 start() 方法启动新线程，即它调用了线程的 run() 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">exitFlag = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myThread</span> <span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, threadID, name, counter)</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.threadID = threadID</span><br><span class="line">        self.name = name</span><br><span class="line">        self.counter = counter</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span>     <span class="comment">#启动线程后执行</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"开始线程："</span> + self.name)</span><br><span class="line">        print_time(self.name, self.counter, <span class="number">5</span>)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"退出线程："</span> + self.name)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_time</span><span class="params">(threadName, delay, counter)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> counter:</span><br><span class="line">        <span class="keyword">if</span> exitFlag:</span><br><span class="line">            threadName.exit()</span><br><span class="line">        time.sleep(delay)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"%s: %s"</span> % (threadName, time.ctime(time.time())))</span><br><span class="line">        counter -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建新线程</span></span><br><span class="line">thread1 = myThread(<span class="number">1</span>, <span class="string">"Thread-1"</span>, <span class="number">1</span>)</span><br><span class="line">thread2 = myThread(<span class="number">2</span>, <span class="string">"Thread-2"</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启新线程</span></span><br><span class="line">thread1.start()</span><br><span class="line">thread2.start()</span><br><span class="line">thread1.join()  <span class="comment">#用于等待线程结束，在线程结束前阻塞执行，有效阻止主线程在所有子线程都完成之前继续执行。</span></span><br><span class="line">thread2.join()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"退出主线程"</span>)</span><br></pre></td></tr></table></figure><h1 id="线程同步"><a href="#线程同步" class="headerlink" title="线程同步"></a>线程同步</h1><p>多线程的优势在于可以同时运行多个任务（至少感觉起来是这样）。但是当线程需要共享数据时，可能存在数据不同步的问题。如果多个线程共同对某个数据修改，则可能出现不可预料的结果，为了保证数据的正确性，需要对多个线程进行同步。</p><p>使用 Thread 对象的 Lock 和 Rlock 可以实现简单的线程同步，这两个对象都有 acquire 方法和 release 方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到 acquire 和 release 方法之间。如下：</p><p>考虑这样一种情况：一个列表里所有元素都是0，线程”set”从后向前把所有元素改成1，而线程”print”负责从前往后读取列表并打印。</p><p>那么，可能线程”set”开始改的时候，线程”print”便来打印列表了，输出就成了一半0一半1，这就是数据的不同步。为了避免这种情况，引入了锁的概念。</p><p>锁有两种状态——锁定和未锁定。每当一个线程比如”set”要访问共享数据时，必须先获得锁定；如果已经有别的线程比如”print”获得锁定了，那么就让线程”set”暂停，也就是同步阻塞；等到线程”print”访问完毕，释放锁以后，再让线程”set”继续。</p><p>经过这样的处理，打印列表时要么全部输出0，要么全部输出1，不会再出现一半0一半1的尴尬场面。</p><p>实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myThread</span> <span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, threadID, name, counter)</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.threadID = threadID</span><br><span class="line">        self.name = name</span><br><span class="line">        self.counter = counter</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"开启线程： "</span> + self.name)</span><br><span class="line">        <span class="comment"># 获取锁，用于线程同步</span></span><br><span class="line">        threadLock.acquire()</span><br><span class="line">        print_time(self.name, self.counter, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 释放锁，开启下一个线程</span></span><br><span class="line">        threadLock.release()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_time</span><span class="params">(threadName, delay, counter)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> counter:</span><br><span class="line">        time.sleep(delay)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"%s: %s"</span> % (threadName, time.ctime(time.time())))</span><br><span class="line">        counter -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">threadLock = threading.Lock()</span><br><span class="line">threads = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建新线程</span></span><br><span class="line">thread1 = myThread(<span class="number">1</span>, <span class="string">"Thread-1"</span>, <span class="number">1</span>)</span><br><span class="line">thread2 = myThread(<span class="number">2</span>, <span class="string">"Thread-2"</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启新线程</span></span><br><span class="line">thread1.start()</span><br><span class="line">thread2.start()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加线程到线程列表</span></span><br><span class="line">threads.append(thread1)</span><br><span class="line">threads.append(thread2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待所有线程完成</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">    t.join()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"退出主线程"</span>)</span><br></pre></td></tr></table></figure><p>执行结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">开启线程： Thread-1</span><br><span class="line">开启线程： Thread-2</span><br><span class="line">Thread-2: Mon Aug 12 16:08:59 2019</span><br><span class="line">Thread-2: Mon Aug 12 16:09:01 2019</span><br><span class="line">Thread-2: Mon Aug 12 16:09:03 2019</span><br><span class="line">Thread-1: Mon Aug 12 16:09:04 2019</span><br><span class="line">Thread-1: Mon Aug 12 16:09:05 2019</span><br><span class="line">Thread-1: Mon Aug 12 16:09:06 2019</span><br><span class="line">退出主线程</span><br></pre></td></tr></table></figure><h2 id="线程优先级队列（-Queue）"><a href="#线程优先级队列（-Queue）" class="headerlink" title="线程优先级队列（ Queue）"></a>线程优先级队列（ Queue）</h2><p>Python 的 Queue 模块中提供了同步的、线程安全的队列类，包括FIFO（先入先出)队列Queue，LIFO（后入先出）队列LifoQueue，和优先级队列 PriorityQueue。</p><p>这些队列都实现了锁原语，能够在多线程中直接使用，可以使用队列来实现线程间的同步。</p><p>Queue 模块中的常用方法:</p><ul><li>Queue.qsize() 返回队列的大小</li><li>Queue.empty() 如果队列为空，返回True,反之False</li><li>Queue.full() 如果队列满了，返回True,反之False</li><li>Queue.full 与 maxsize 大小对应</li><li>Queue.get([block[, timeout]])获取队列，timeout等待时间</li><li>Queue.get_nowait() 相当Queue.get(False)</li><li>Queue.put(item) 写入队列，timeout等待时间</li><li>Queue.put_nowait(item) 相当Queue.put(item, False)</li><li>Queue.task_done() 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号</li><li>Queue.join() 实际上意味着等到队列为空，再执行别的操作</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">exitFlag = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myThread</span> <span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, threadID, name, q)</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.threadID = threadID</span><br><span class="line">        self.name = name</span><br><span class="line">        self.q = q</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"开启线程："</span> + self.name)</span><br><span class="line">        process_data(self.name, self.q)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"退出线程："</span> + self.name)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_data</span><span class="params">(threadName, q)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> exitFlag:</span><br><span class="line">        queueLock.acquire()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> workQueue.empty():</span><br><span class="line">            data = q.get()</span><br><span class="line">            queueLock.release()</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"%s processing %s"</span> % (threadName, data))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            queueLock.release()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">threadList = [<span class="string">"Thread-1"</span>, <span class="string">"Thread-2"</span>, <span class="string">"Thread-3"</span>]</span><br><span class="line">nameList = [<span class="string">"One"</span>, <span class="string">"Two"</span>, <span class="string">"Three"</span>, <span class="string">"Four"</span>, <span class="string">"Five"</span>]</span><br><span class="line">queueLock = threading.Lock()</span><br><span class="line">workQueue = queue.Queue(<span class="number">10</span>)</span><br><span class="line">threads = []</span><br><span class="line">threadID = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建新线程</span></span><br><span class="line"><span class="keyword">for</span> tName <span class="keyword">in</span> threadList:</span><br><span class="line">    thread = myThread(threadID, tName, workQueue)</span><br><span class="line">    thread.start()</span><br><span class="line">    threads.append(thread)</span><br><span class="line">    threadID += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充队列</span></span><br><span class="line">queueLock.acquire()</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> nameList:</span><br><span class="line">    workQueue.put(word)</span><br><span class="line">queueLock.release()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待队列清空</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> workQueue.empty():</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通知线程是时候退出</span></span><br><span class="line">exitFlag = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待所有线程完成</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">    t.join()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"退出主线程"</span>)</span><br></pre></td></tr></table></figure><p>执行结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">开启线程：Thread-1</span><br><span class="line">开启线程：Thread-2</span><br><span class="line">开启线程：Thread-3</span><br><span class="line">Thread-3 processing One</span><br><span class="line">Thread-1 processing Two</span><br><span class="line">Thread-2 processing Three</span><br><span class="line">Thread-3 processing Four</span><br><span class="line">Thread-1 processing Five</span><br><span class="line">退出线程：Thread-3</span><br><span class="line">退出线程：Thread-2</span><br><span class="line">退出线程：Thread-1</span><br><span class="line">退出主线程</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;多线程类似于同时执行多个不同程序，多线程运行有如下优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用线程可以把占据长时间的程序中的任务放到后台去处理。&lt;/li&gt;
&lt;li&gt;程序的运行速度可能加快。&lt;/li&gt;
&lt;li&gt;对于I/O密集型Python程序如用户输入、文件读写和网络收发数据等，线
      
    
    </summary>
    
      <category term="python" scheme="https://hiworldalice.github.io/categories/python/"/>
    
    
      <category term="多线程" scheme="https://hiworldalice.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达机器学习作业Python实现(三)：多分类&amp;神经网络</title>
    <link href="https://hiworldalice.github.io/2019/08/11/%E5%A4%9A%E5%88%86%E7%B1%BB&amp;%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://hiworldalice.github.io/2019/08/11/多分类&amp;神经网络/</id>
    <published>2019-08-11T12:33:34.000Z</published>
    <updated>2019-08-15T14:31:21.326Z</updated>
    
    <content type="html"><![CDATA[<h1 id="多分类Logistic回归"><a href="#多分类Logistic回归" class="headerlink" title="多分类Logistic回归"></a>多分类Logistic回归</h1><p>在本练习中，你将使用逻辑回归和神经网络来识别手写数字(从0到9)。这个练习将向你展示如何将您所学的方法用于此分类任务。在练习的第一部分中，你将扩展以前的逻辑回归实现，并将其应用于one-vs-all分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br></pre></td></tr></table></figure><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>首先，加载数据集。这里的数据为MATLAB的格式，所以要使用SciPy.io的loadmat函数。读取data = loadmat(path)，注意这里data是一个dict数据结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = loadmat(path)</span><br><span class="line">    X = data[<span class="string">'X'</span>]</span><br><span class="line">    y = data[<span class="string">'y'</span>]</span><br><span class="line">    <span class="keyword">return</span> X,y</span><br><span class="line"></span><br><span class="line">X, y = load_data(<span class="string">'ex3data1.mat'</span>)</span><br><span class="line">print(np.unique(y))<span class="comment">#看下有几类标签</span></span><br><span class="line">print(X.shape, y.shape)</span><br></pre></td></tr></table></figure><pre><code>[ 1  2  3  4  5  6  7  8  9 10](5000, 400) (5000, 1)</code></pre><p>ex3data1中有5000个训练示例。mat，其中每个训练实例是一个20像素×20像素灰度图像的数字。每个像素由表示该位置灰度强度的发泡点数表示。20×20像素的网格被“展开”成一个400维的向量。这些训练示例中的每一个都变成数据矩阵X中的一行。这就得到了一个5000×400的矩阵X，其中每一行都是一个手写数字图像的训练示例。</p><h2 id="Visualizing-the-data"><a href="#Visualizing-the-data" class="headerlink" title="Visualizing the data"></a>Visualizing the data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_an_image</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    随机打印一个数字</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pick_one = np.random.randint(<span class="number">0</span>, <span class="number">5000</span>)</span><br><span class="line">    image = X[pick_one, :]</span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    ax.matshow(image.reshape((<span class="number">20</span>, <span class="number">20</span>)), cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">    plt.xticks([])  <span class="comment"># 去除刻度，美观</span></span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">'this should be &#123;&#125;'</span>.format(y[pick_one]))</span><br><span class="line">    </span><br><span class="line">plot_an_image(X)</span><br></pre></td></tr></table></figure><p><img src="/images/8.11_1.png" alt="png"></p><pre><code>this should be [8]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_100_image</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string">    随机画100个数字</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    sample_idx = np.random.choice(np.arange(X.shape[<span class="number">0</span>]), <span class="number">100</span>)  <span class="comment"># 随机选100个样本</span></span><br><span class="line">    sample_images = X[sample_idx, :]  <span class="comment"># (100,400)</span></span><br><span class="line">    </span><br><span class="line">    fig, ax_array = plt.subplots(nrows=<span class="number">10</span>, ncols=<span class="number">10</span>, sharey=<span class="literal">True</span>, sharex=<span class="literal">True</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> column <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            ax_array[row, column].matshow(sample_images[<span class="number">10</span> * row + column].reshape((<span class="number">20</span>, <span class="number">20</span>)),</span><br><span class="line">                                   cmap=<span class="string">'gray_r'</span>)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])        </span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_100_image(X)</span><br></pre></td></tr></table></figure><p><img src="/images/8.11_2.png" alt="png"></p><h2 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h2><p>我们将使用多个one-vs-all(一对多)logistic回归模型来构建一个多类分类器。由于有10个类，需要训练10个独立的分类器。为了提高训练效率，重要的是向量化。在本节中，我们将实现一个不使用任何for循环的向量化的logistic回归版本。</p><h3 id="Vectorizing-the-cost-function"><a href="#Vectorizing-the-cost-function" class="headerlink" title="Vectorizing the cost function"></a>Vectorizing the cost function</h3><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^m{[y^{(i)}log(h_\theta(x^{(i)}))-{(1-y^{(i)})}{log(1-h_\theta(x^{(i)}))]}}</script><p>注意：不惩罚第一项$\theta_0$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost</span><span class="params">(theta, X, y, l)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    don't penalize theta_0</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        X: feature matrix, (m, n+1) # 插入了x0=1</span></span><br><span class="line"><span class="string">        y: target vector, (m, )</span></span><br><span class="line"><span class="string">        l: lambda constant for regularization</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    thetaReg = theta[<span class="number">1</span>:]</span><br><span class="line">    first = (-y*np.log(sigmoid(X@theta))) + (y<span class="number">-1</span>)*np.log(<span class="number">1</span>-sigmoid(X@theta))</span><br><span class="line">    reg = (thetaReg@thetaReg)*l / (<span class="number">2</span>*len(X))</span><br><span class="line">    <span class="keyword">return</span> np.mean(first) + reg</span><br></pre></td></tr></table></figure><h3 id="Vectorizing-the-gradient"><a href="#Vectorizing-the-gradient" class="headerlink" title="Vectorizing the gradient"></a>Vectorizing the gradient</h3><p>正则化梯度下降：</p><script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}{x_0}^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m[{({h_\theta(x^{(i)})}-y^{(i)})}{x_j}^{(i)}+\frac{\lambda}{m}\theta_j],for j=1,2,...n</script><p>对上面的算法中 j=1,2,…,n 时的更新式子进行调整可得：</p><script type="math/tex; mode=display">\theta_j:=\theta_j(1-a\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}{x_j}^{(i)}</script><p>仅需要计算出梯度：<script type="math/tex">\frac{\partial}{\partial\theta_j}J(\theta)</script></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span><span class="params">(theta, X, y, l)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    don't penalize theta_0</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        l: lambda constant</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        a vector of gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    thetaReg = theta[<span class="number">1</span>:]</span><br><span class="line">    first = (<span class="number">1</span> / len(X)) * X.T @ (sigmoid(X @ theta) - y)</span><br><span class="line">    <span class="comment"># 这里人为插入一维0，使得对theta_0不惩罚，方便计算</span></span><br><span class="line">    reg = np.concatenate([np.array([<span class="number">0</span>]), (l / len(X)) * thetaReg])</span><br><span class="line">    <span class="keyword">return</span> first + reg</span><br></pre></td></tr></table></figure><h2 id="One-vs-all-Classification"><a href="#One-vs-all-Classification" class="headerlink" title="One-vs-all Classification"></a>One-vs-all Classification</h2><p>这部分我们将实现一对多分类通过训练多个正则化logistic回归分类器，每个对应数据集中K类中的一个。</p><p>对于这个任务，我们有10个可能的类，并且由于logistic回归只能一次在2个类之间进行分类，每个分类器在“类别 i”和“不是 i”之间决定。 我们将把分类器训练包含在一个函数中，该函数计算10个分类器中的每个分类器的最终权重，并将权重返回shape为(k, (n+1))数组，其中 n 是参数数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_vs_all</span><span class="params">(X, y, l, K)</span>:</span></span><br><span class="line">    <span class="string">"""generalized logistic regression</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        X: feature matrix, (m, n+1) # with incercept x0=1</span></span><br><span class="line"><span class="string">        y: target vector, (m, )</span></span><br><span class="line"><span class="string">        l: lambda constant for regularization</span></span><br><span class="line"><span class="string">        K: numbel of labels</span></span><br><span class="line"><span class="string">    return: trained parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    all_theta = np.zeros((K, X.shape[<span class="number">1</span>]))  <span class="comment"># (10, 401)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, K+<span class="number">1</span>):</span><br><span class="line">        theta = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">        y_i = np.array([<span class="number">1</span> <span class="keyword">if</span> label == i <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> label <span class="keyword">in</span> y])</span><br><span class="line">    </span><br><span class="line">        ret = minimize(fun=regularized_cost, x0=theta, args=(X, y_i, l), method=<span class="string">'TNC'</span>,</span><br><span class="line">                        jac=regularized_gradient, options=&#123;<span class="string">'disp'</span>: <span class="literal">True</span>&#125;)</span><br><span class="line">        all_theta[i<span class="number">-1</span>,:] = ret.x                         </span><br><span class="line">    <span class="keyword">return</span> all_theta</span><br></pre></td></tr></table></figure><p>这里需要注意的几点：首先，我们为X添加了一列常数项 1 ，以计算截距项（常数项）。 其次，我们将y从类标签转换为每个分类器的二进制值（要么是类i，要么不是类i）。 最后，我们使用SciPy的较新优化API来最小化每个分类器的代价函数。 如果指定的话，API将采用目标函数，初始参数集，优化方法和jacobian（渐变）函数。 然后将优化程序找到的参数分配给参数数组。</p><p>实现向量化代码的一个更具挑战性的部分是正确地写入所有的矩阵，保证维度正确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_all</span><span class="params">(X, all_theta)</span>:</span></span><br><span class="line">    <span class="comment"># compute the class probability for each class on each training instance   </span></span><br><span class="line">    h = sigmoid(X @ all_theta.T)  <span class="comment"># 注意的这里的all_theta需要转置</span></span><br><span class="line">    <span class="comment"># create array of the index with the maximum probability</span></span><br><span class="line">    <span class="comment"># Returns the indices of the maximum values along an axis.</span></span><br><span class="line">    h_argmax = np.argmax(h, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># because our array was zero-indexed we need to add one for the true label prediction</span></span><br><span class="line">    h_argmax = h_argmax + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> h_argmax</span><br></pre></td></tr></table></figure><p>这里的h共5000行，10列，每行代表一个样本，每列是预测对应数字的概率。我们取概率最大对应的index加1就是我们分类器最终预测出来的类别。返回的h_argmax是一个array，包含5000个样本对应的预测值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">raw_X, raw_y = load_data(<span class="string">'ex3data1.mat'</span>)</span><br><span class="line">X = np.insert(raw_X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>) <span class="comment"># (5000, 401)</span></span><br><span class="line">y = raw_y.flatten()  <span class="comment"># 这里消除了一个维度，方便后面的计算 or .reshape(-1) （5000，）</span></span><br><span class="line"></span><br><span class="line">all_theta = one_vs_all(X, y, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">print(all_theta)  <span class="comment"># 每一行是一个分类器的一组参数</span></span><br><span class="line"></span><br><span class="line">y_pred = predict_all(X, all_theta)</span><br><span class="line">accuracy = np.mean(y_pred == y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'accuracy = &#123;0&#125;%'</span>.format(accuracy * <span class="number">100</span>))</span><br></pre></td></tr></table></figure><pre><code>[[-2.38128531e+00  0.00000000e+00  0.00000000e+00 ...  1.30458708e-03  -7.92437494e-10  0.00000000e+00] [-3.18322512e+00  0.00000000e+00  0.00000000e+00 ...  4.46099360e-03  -5.08609140e-04  0.00000000e+00] [-4.79832764e+00  0.00000000e+00  0.00000000e+00 ... -2.87304710e-05  -2.47686612e-07  0.00000000e+00] ... [-7.98611966e+00  0.00000000e+00  0.00000000e+00 ... -8.95219138e-05   7.22466594e-06  0.00000000e+00] [-4.57371149e+00  0.00000000e+00  0.00000000e+00 ... -1.33375266e-03   9.95904709e-05  0.00000000e+00] [-5.40461055e+00  0.00000000e+00  0.00000000e+00 ... -1.16591716e-04   7.87801369e-06  0.00000000e+00]]accuracy = 94.46%</code></pre><h1 id="Neutural-Networks"><a href="#Neutural-Networks" class="headerlink" title="Neutural Networks"></a>Neutural Networks</h1><p>上面使用了多类logistic回归，然而logistic回归不能形成更复杂的假设，因为它只是一个线性分类器。</p><p>接下来我们用神经网络来尝试下，神经网络可以实现非常复杂的非线性的模型。我们将利用已经训练好了的权重进行预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = loadmat(path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'Theta1'</span>], data[<span class="string">'Theta2'</span>]</span><br><span class="line"></span><br><span class="line">theta1, theta2 = load_weight(<span class="string">'ex3weights.mat'</span>)</span><br><span class="line"></span><br><span class="line">print(theta1.shape, theta2.shape)</span><br></pre></td></tr></table></figure><pre><code>(25, 401) (10, 26)</code></pre><p>因此在数据加载函数中，原始数据做了转置，然而，转置的数据与给定的参数不兼容，因为这些参数是由原始数据训练的。 所以为了应用给定的参数，我需要使用原始数据（不转置）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, y = load_data(<span class="string">'ex3data1.mat'</span>)</span><br><span class="line">y = y.flatten()</span><br><span class="line">X = np.insert(X, <span class="number">0</span>, values=np.ones(X.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)  <span class="comment"># intercept</span></span><br><span class="line"></span><br><span class="line">print(X.shape, y.shape)</span><br></pre></td></tr></table></figure><pre><code>(5000, 401) (5000,)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a1 = X</span><br><span class="line">z2 = a1 @ theta1.T</span><br><span class="line">z2 = np.insert(z2, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">a2 = sigmoid(z2)</span><br><span class="line"></span><br><span class="line">z3 = a2 @ theta2.T</span><br><span class="line">a3 = sigmoid(z3)</span><br><span class="line"></span><br><span class="line">y_pred = np.argmax(a3, axis=<span class="number">1</span>) + <span class="number">1</span> </span><br><span class="line">accuracy = np.mean(y_pred == y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'accuracy = &#123;0&#125;%'</span>.format(accuracy * <span class="number">100</span>))</span><br></pre></td></tr></table></figure><pre><code>accuracy = 97.52%</code></pre><p>虽然人工神经网络是非常强大的模型，但训练数据的准确性并不能完美预测实际数据，在这里很容易过拟合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;多分类Logistic回归&quot;&gt;&lt;a href=&quot;#多分类Logistic回归&quot; class=&quot;headerlink&quot; title=&quot;多分类Logistic回归&quot;&gt;&lt;/a&gt;多分类Logistic回归&lt;/h1&gt;&lt;p&gt;在本练习中，你将使用逻辑回归和神经网络来识别手写数字
      
    
    </summary>
    
      <category term="python" scheme="https://hiworldalice.github.io/categories/python/"/>
    
    
      <category term="机器学习" scheme="https://hiworldalice.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达机器学习作业Python实现(二)：Logistic回归</title>
    <link href="https://hiworldalice.github.io/2019/08/05/Logistic%E5%9B%9E%E5%BD%92/"/>
    <id>https://hiworldalice.github.io/2019/08/05/Logistic回归/</id>
    <published>2019-08-05T09:10:34.000Z</published>
    <updated>2019-08-05T09:37:57.821Z</updated>
    
    <content type="html"><![CDATA[<p>在这部分练习中，你将建立一个logistics回归模型来预测一个学生是否能被大学录取。假如你是大学招生办的工作人员，你想通过学生的两次考试成绩来决定他被录取的概率。你有一些往届学生的历史数据作为逻辑回归的训练集，对于每一个训练样本，你有学生两次考试的分数和录取结果。你的任务是建立一个分类模型来估计每个学生基于两次考试成绩被录取的概率。</p><h1 id="Logistics回归"><a href="#Logistics回归" class="headerlink" title="Logistics回归"></a>Logistics回归</h1><h2 id="导入数据并查看"><a href="#导入数据并查看" class="headerlink" title="导入数据并查看"></a>导入数据并查看</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line"></span><br><span class="line">path = <span class="string">r'ex2data1.txt'</span></span><br><span class="line">data = pd.read_csv(path,names=[<span class="string">'exam1'</span>,<span class="string">'exam2'</span>,<span class="string">'admitted'</span>])</span><br><span class="line">print(data.head())</span><br></pre></td></tr></table></figure><pre><code>       exam1      exam2  admitted0  34.623660  78.024693         01  30.286711  43.894998         02  35.847409  72.902198         03  60.182599  86.308552         14  79.032736  75.344376         1</code></pre><p>绘制散点图，横坐标为exam1的分数，纵坐标为exam2的分数，被录取则样本为正，没有被录取则样本为负，用颜色样式加以区分。<br>pandas库中的isin()函数用于数据筛选，isin()接受一个列表，判断该列中元素是否在列表中，多用于要选择某列等于多个数值或者字符串时。data[data[‘admitted’].isin([‘1’])]选取admitted列值为1的所有行，等价于data[data[‘admitted’]==1]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">positive = data[data.admitted.isin([<span class="string">'1'</span>])]  <span class="comment"># 1</span></span><br><span class="line">negetive = data[data.admitted.isin([<span class="string">'0'</span>])]  <span class="comment"># 0</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line">ax.scatter(positive[<span class="string">'exam1'</span>], positive[<span class="string">'exam2'</span>], c=<span class="string">'b'</span>, label=<span class="string">'Admitted'</span>)</span><br><span class="line">ax.scatter(negetive[<span class="string">'exam1'</span>], negetive[<span class="string">'exam2'</span>], s=<span class="number">50</span>, c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>, label=<span class="string">'Unadmitted'</span>)</span><br><span class="line">ax.legend()<span class="comment">#默认将图例调整到最佳区域</span></span><br><span class="line"><span class="comment"># 设置横纵坐标名</span></span><br><span class="line">ax.set_xlabel(<span class="string">'Exam 1 Score'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Exam 2 Score'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/8.5_1.png" alt="png"></p><h2 id="实现代价函数-变量初始化"><a href="#实现代价函数-变量初始化" class="headerlink" title="实现代价函数,变量初始化"></a>实现代价函数,变量初始化</h2><p>logistic回归的假设函数为<script type="math/tex">h_\theta=g(\theta^TX)</script>其中g是一个S型函数(Sigmoid function)</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>逻辑回归的代价函数如下，这个函数是凸的。</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^m{[y^{(i)}log(h_\theta(x^{(i)}))-{(1-y^{(i)})}{log(1-h_\theta(x^{(i)}))]}}</script><p>此过程应用向量化方法，矩阵相乘时需要考虑矩阵的维度，必要时要对矩阵进行转置。注意矩阵matrix和数组array的区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(theta, X, Y)</span>:</span></span><br><span class="line">    first = (-Y) * np.log(Sigmoid(X @ theta))</span><br><span class="line">    second = (<span class="number">1</span> - Y)*np.log(<span class="number">1</span> - Sigmoid(X @ theta))</span><br><span class="line">    <span class="keyword">return</span> np.mean(first - second)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add a ones column - this makes the matrix multiplication work out easier</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">'Ones'</span> <span class="keyword">not</span> <span class="keyword">in</span> data.columns:</span><br><span class="line">    data.insert(<span class="number">0</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># set X (training data) and y (target variable)</span></span><br><span class="line">X = data.iloc[:, :<span class="number">-1</span>].values  <span class="comment"># Convert the frame to its Numpy-array representation.</span></span><br><span class="line">Y = data.iloc[:, <span class="number">-1</span>].values  <span class="comment"># Return is NOT a Numpy-matrix, rather, a Numpy-array.</span></span><br><span class="line">theta = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">print(computeCost(theta, X, Y))</span><br></pre></td></tr></table></figure><pre><code>0.6931471805599453</code></pre><h2 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h2><p>bathch gradient decent(批量梯度下降)</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}{x_j}^{(i)}}{m}</script><p>实现梯度下降过程中(如下)程序报错ValueError: shapes (100,1) and (3,100) not aligned: 1 (dim 1) != 3 (dim 0),实在不知道哪里出错了，多次检查矩阵乘法未果，遂放弃梯度下降算法。在此只计算出梯度：<script type="math/tex">\frac{\partial}{\partial\theta_j}J(\theta)</script></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">def gradientDescent(X, Y, theta, alpha, epoch):</span></span><br><span class="line"><span class="string">    temp = np.matrix(np.zeros(theta.shape))  # 初始化一个临时矩阵(1, 3)</span></span><br><span class="line"><span class="string">    cost = np.zeros(epoch)                   # 初始化一个ndarray，包含每次epoch的cost</span></span><br><span class="line"><span class="string">    m = X.shape[0]                           # m为总的样本数</span></span><br><span class="line"><span class="string">    #利用向量化一步求解</span></span><br><span class="line"><span class="string">    for i in range(epoch):</span></span><br><span class="line"><span class="string">        temp = theta - (alpha / m) * (Sigmoid(X * theta.T)-Y).T * X</span></span><br><span class="line"><span class="string">        theta = temp</span></span><br><span class="line"><span class="string">        cost[i] = computeCost(X,Y,theta) #得到每次迭代代价函数的值</span></span><br><span class="line"><span class="string">    return theta,cost</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#初始化学习率α和要进行迭代的次数</span></span><br><span class="line"><span class="string">alpha = 0.01</span></span><br><span class="line"><span class="string">epoch = 1000</span></span><br><span class="line"><span class="string">#现在让我们运行梯度下降算法来将我们的参数θ适合于训练集</span></span><br><span class="line"><span class="string">final_theta,cost = gradientDescent(X,Y,theta,alpha,epoch)</span></span><br><span class="line"><span class="string">print(cost)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (X.T @ (Sigmoid(X @ theta) - y))/len(X)   </span><br><span class="line"><span class="comment"># the gradient of the cost is a vector of the same length as θ where the jth element (for j = 0, 1, . . . , n)</span></span><br><span class="line">print(gradient(theta, X, Y))</span><br></pre></td></tr></table></figure><pre><code>[ -0.1        -12.00921659 -11.26284221]</code></pre><h2 id="运用Scipy优化算法来最小化代价函数，学习参数-theta"><a href="#运用Scipy优化算法来最小化代价函数，学习参数-theta" class="headerlink" title="运用Scipy优化算法来最小化代价函数，学习参数$\theta$"></a>运用Scipy优化算法来最小化代价函数，学习参数$\theta$</h2><p>由于上面并没有实现梯度下降算法，只是计算出梯度。在练习中，一个称为“fminunc”的Octave函数是用来优化函数来计算成本和梯度参数。由于我们使用Python，我们可以用SciPy的“optimize”命名空间来做同样的事情。scipy中的optimize子包中提供了常用的最优化算法函数实现，我们可以直接调用这些函数完成我们的优化问题。在用python实现逻辑回归和线性回归时，使用梯度下降法最小化cost function，用到了fmin_tnc()和minimize()。此处参考<a href="https://www.cnblogs.com/tongtong123/p/10634716.html" target="_blank" rel="noopener">https://www.cnblogs.com/tongtong123/p/10634716.html</a><br>这里我们使用的是高级优化算法，运行速度通常远远超过梯度下降。方便快捷。<br>只需传入cost函数，已经所求的变量theta，和梯度。cost函数定义变量时变量tehta要放在第一个，若cost函数只返回cost，则设置fprime=gradient。</p><h3 id="1-fmin-tnc"><a href="#1-fmin-tnc" class="headerlink" title="1.fmin_tnc()"></a>1.fmin_tnc()</h3><p>scipy.optimize.fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0)，这里只列举出常用参数，详细可参考官方文档<a href="https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.optimize.fmin_tnc.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.optimize.fmin_tnc.html</a></p><ul><li>参数：<br>func：优化的目标函数（此例中为代价函数）<br>x0(array_like)：初值（此例中为$\theta$）<br>fprime：提供优化函数func的梯度函数，不然优化函数func必须返回函数值和梯度，或者设置approx_grad=True<br>approx_grad :如果设置为True，会给出近似梯度<br>args：元组，是传递给优化函数的参数</li><li>返回值：<br>x(ndarray): The solution.<br>nfeval(int): The number of function evaluations.<br>rc(int): Return code, see below</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line">result = opt.fmin_tnc(func=computeCost, x0=theta, fprime=gradient, args=(X, Y))</span><br><span class="line">print(result[<span class="number">0</span>])   <span class="comment">#返回代价函数取最小值时的参数值</span></span><br></pre></td></tr></table></figure><pre><code>[-25.1613186    0.20623159   0.20147149]</code></pre><h3 id="2-minimize"><a href="#2-minimize" class="headerlink" title="2.minimize()"></a>2.minimize()</h3><p>除了用fmin_tnc()方法，还可以用minimize()方法，<br>scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None)，这里只列举出常用参数，详细可参考官方文档<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html</a></p><ul><li>参数：<br>fun ：优化的目标函数<br>x0 ：初值，一维数组，shape (n,)<br>args ： 元组，可选，额外传递给优化函数的参数<br>method：求解的算法，选择TNC则和fmin_tnc()类似<br>jac：返回梯度向量的函数</li><li>返回值：<br>返回优化结果对象，x：优化问题的目标数组。success: True表示成功与否，不成功会给出失败信息。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res = opt.minimize(fun=computeCost, x0=theta, args=(X, Y), method=<span class="string">'TNC'</span>, jac=gradient)</span><br><span class="line">print(res.x)</span><br></pre></td></tr></table></figure><pre><code>[-25.1613186    0.20623159   0.20147149]</code></pre><h2 id="输出预测值及被录取（输出值为1）的概率"><a href="#输出预测值及被录取（输出值为1）的概率" class="headerlink" title="输出预测值及被录取（输出值为1）的概率"></a>输出预测值及被录取（输出值为1）的概率</h2><p>逻辑回归模型的假设函数：<script type="math/tex">h_\theta(x)=\frac{1}{1+e^{\theta^TX}}</script><br>$h<em>\theta(x)$的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1 的可能性。<br>当$h</em>\theta(x)$小于0.5时，预测 y = 0<br>当$h_\theta(x)$大于0.5时，预测 y = 1<br>接下来，我们需要编写一个函数，用我们所学的参数theta来为数据集X输出预测。然后，我们可以使用这个函数来给我们的分类器的训练精度打分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对样本集进行预测，得到一个预测值列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    probability = Sigmoid(X@theta)</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x &gt;= <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> probability]  <span class="comment"># return a list</span></span><br><span class="line"></span><br><span class="line">final_theta = result[<span class="number">0</span>]</span><br><span class="line">predictions = predict(final_theta, X)</span><br><span class="line">correct = [<span class="number">1</span> <span class="keyword">if</span> a==b <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, Y)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算预测精度</span></span><br><span class="line">accuracy = sum(correct) / len(X)</span><br><span class="line">print(accuracy)</span><br></pre></td></tr></table></figure><pre><code>0.89</code></pre><h2 id="绘制决策边界"><a href="#绘制决策边界" class="headerlink" title="绘制决策边界"></a>绘制决策边界</h2><p>分隔 y=0 的区域和 y=1 的区域的曲线即为决策边界，在这个假设模型中是$\theta_0+\theta_1x_1+\theta_2x_2=0$所对应的曲线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x1 = np.arange(<span class="number">130</span>, step=<span class="number">0.1</span>)</span><br><span class="line">x2 = -(final_theta[<span class="number">0</span>] + x1*final_theta[<span class="number">1</span>]) / final_theta[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">ax.scatter(positive[<span class="string">'exam1'</span>], positive[<span class="string">'exam2'</span>], c=<span class="string">'b'</span>, label=<span class="string">'Admitted'</span>)</span><br><span class="line">ax.scatter(negetive[<span class="string">'exam1'</span>], negetive[<span class="string">'exam2'</span>], s=<span class="number">50</span>, c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>, label=<span class="string">'Not Admitted'</span>)</span><br><span class="line">ax.plot(x1, x2)</span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">130</span>)</span><br><span class="line">ax.set_ylim(<span class="number">0</span>, <span class="number">130</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'x1'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'x2'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Decision Boundary'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/8.5_2.png" alt="png"></p><h1 id="正则化Logistic回归"><a href="#正则化Logistic回归" class="headerlink" title="正则化Logistic回归"></a>正则化Logistic回归</h1><p>在训练的第二部分，我们将要通过加入正则项提升逻辑回归算法。简而言之，正则化是成本函数中的一个术语，它使算法更倾向于“更简单”的模型（在这种情况下，模型将更小的系数）。这个理论助于减少过拟合，提高模型的泛化能力。<br>在练习的这一部分中，你将实现正则化的逻辑回归，以预测来自制造工厂的微芯片是否通过质量保证(QA)。在这过程中，每个芯片都要经过各种测试，以确保其正常工作。假设你是工厂的产品经理，你有一些微芯片在两种不同的测试上的测试结果。从这两个测试中，你想确定应该接受还是拒绝微芯片。为了帮助你做出决策，你有一个关于过去微芯片的测试结果的数据集，你可以从中构建一个逻辑回归模型。</p><h2 id="导入数据并查看-1"><a href="#导入数据并查看-1" class="headerlink" title="导入数据并查看"></a>导入数据并查看</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data2 = pd.read_csv(<span class="string">'ex2data2.txt'</span>, names=[<span class="string">'Test 1'</span>, <span class="string">'Test 2'</span>, <span class="string">'Accepted'</span>])</span><br><span class="line">data2.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Test 1</th>      <th>Test 2</th>      <th>Accepted</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.051267</td>      <td>0.69956</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>-0.092742</td>      <td>0.68494</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>-0.213710</td>      <td>0.69225</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>-0.375000</td>      <td>0.50219</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>-0.513250</td>      <td>0.46564</td>      <td>1</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">positive = data2[data2[<span class="string">'Accepted'</span>].isin([<span class="number">1</span>])]</span><br><span class="line">negative = data2[data2[<span class="string">'Accepted'</span>].isin([<span class="number">0</span>])]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">ax.scatter(positive[<span class="string">'Test 1'</span>], positive[<span class="string">'Test 2'</span>], s=<span class="number">50</span>, c=<span class="string">'b'</span>, marker=<span class="string">'o'</span>, label=<span class="string">'Accepted'</span>)</span><br><span class="line">ax.scatter(negative[<span class="string">'Test 1'</span>], negative[<span class="string">'Test 2'</span>], s=<span class="number">50</span>, c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>, label=<span class="string">'Rejected'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.set_xlabel(<span class="string">'Test 1 Score'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Test 2 Score'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/8.5_3.png" alt="png"></p><p>注意到正负样本之间没有线性的决策边界，所以直接用Logistic回归在这个数据集上并不能表现良好。</p><h2 id="生成多项式特征"><a href="#生成多项式特征" class="headerlink" title="生成多项式特征"></a>生成多项式特征</h2><p>一个拟合数据的更好的方法是从每个数据点创建更多的特征。<br>我们将把这些特征映射到所有的x1和x2的多项式项上，直到第六次幂。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_mapping</span><span class="params">(x1, x2, power)</span>:</span></span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(power + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> np.arange(i + <span class="number">1</span>):</span><br><span class="line">            data[<span class="string">"f&#123;&#125;&#123;&#125;"</span>.format(i - p, p)] = np.power(x1, i - p) * np.power(x2, p)</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(data)</span><br><span class="line">x1 = data2[<span class="string">'Test 1'</span>].values</span><br><span class="line">x2 = data2[<span class="string">'Test 2'</span>].values</span><br><span class="line">_data2 = feature_mapping(x1, x2, power=<span class="number">6</span>)</span><br><span class="line">_data2.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }    .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>f00</th>      <th>f10</th>      <th>f01</th>      <th>f20</th>      <th>f11</th>      <th>f02</th>      <th>f30</th>      <th>f21</th>      <th>f12</th>      <th>f03</th>      <th>...</th>      <th>f23</th>      <th>f14</th>      <th>f05</th>      <th>f60</th>      <th>f51</th>      <th>f42</th>      <th>f33</th>      <th>f24</th>      <th>f15</th>      <th>f06</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1.0</td>      <td>0.051267</td>      <td>0.69956</td>      <td>0.002628</td>      <td>0.035864</td>      <td>0.489384</td>      <td>0.000135</td>      <td>0.001839</td>      <td>0.025089</td>      <td>0.342354</td>      <td>...</td>      <td>0.000900</td>      <td>0.012278</td>      <td>0.167542</td>      <td>1.815630e-08</td>      <td>2.477505e-07</td>      <td>0.000003</td>      <td>0.000046</td>      <td>0.000629</td>      <td>0.008589</td>      <td>0.117206</td>    </tr>    <tr>      <th>1</th>      <td>1.0</td>      <td>-0.092742</td>      <td>0.68494</td>      <td>0.008601</td>      <td>-0.063523</td>      <td>0.469143</td>      <td>-0.000798</td>      <td>0.005891</td>      <td>-0.043509</td>      <td>0.321335</td>      <td>...</td>      <td>0.002764</td>      <td>-0.020412</td>      <td>0.150752</td>      <td>6.362953e-07</td>      <td>-4.699318e-06</td>      <td>0.000035</td>      <td>-0.000256</td>      <td>0.001893</td>      <td>-0.013981</td>      <td>0.103256</td>    </tr>    <tr>      <th>2</th>      <td>1.0</td>      <td>-0.213710</td>      <td>0.69225</td>      <td>0.045672</td>      <td>-0.147941</td>      <td>0.479210</td>      <td>-0.009761</td>      <td>0.031616</td>      <td>-0.102412</td>      <td>0.331733</td>      <td>...</td>      <td>0.015151</td>      <td>-0.049077</td>      <td>0.158970</td>      <td>9.526844e-05</td>      <td>-3.085938e-04</td>      <td>0.001000</td>      <td>-0.003238</td>      <td>0.010488</td>      <td>-0.033973</td>      <td>0.110047</td>    </tr>    <tr>      <th>3</th>      <td>1.0</td>      <td>-0.375000</td>      <td>0.50219</td>      <td>0.140625</td>      <td>-0.188321</td>      <td>0.252195</td>      <td>-0.052734</td>      <td>0.070620</td>      <td>-0.094573</td>      <td>0.126650</td>      <td>...</td>      <td>0.017810</td>      <td>-0.023851</td>      <td>0.031940</td>      <td>2.780914e-03</td>      <td>-3.724126e-03</td>      <td>0.004987</td>      <td>-0.006679</td>      <td>0.008944</td>      <td>-0.011978</td>      <td>0.016040</td>    </tr>    <tr>      <th>4</th>      <td>1.0</td>      <td>-0.513250</td>      <td>0.46564</td>      <td>0.263426</td>      <td>-0.238990</td>      <td>0.216821</td>      <td>-0.135203</td>      <td>0.122661</td>      <td>-0.111283</td>      <td>0.100960</td>      <td>...</td>      <td>0.026596</td>      <td>-0.024128</td>      <td>0.021890</td>      <td>1.827990e-02</td>      <td>-1.658422e-02</td>      <td>0.015046</td>      <td>-0.013650</td>      <td>0.012384</td>      <td>-0.011235</td>      <td>0.010193</td>    </tr>  </tbody></table><p>5 rows × 28 columns</p></div><p>经过映射，我们将有两个特征的向量转化成了一个28维的向量。<br>在这个高维特征向量上训练的logistic回归分类器将会有一个更复杂的决策边界，当我们在二维图中绘制时，会出现非线性。<br>虽然特征映射允许我们构建一个更有表现力的分类器，但它也更容易过拟合。在接下来的练习中，我们将实现正则化的logistic回归来拟合数据，并且可以看到正则化如何帮助解决过拟合的问题。</p><h2 id="实现正则化代价函数，变量初始化"><a href="#实现正则化代价函数，变量初始化" class="headerlink" title="实现正则化代价函数，变量初始化"></a>实现正则化代价函数，变量初始化</h2><script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^m{[y^{(i)}log(h_\theta(x^{(i)}))-{(1-y^{(i)})}{log(1-h_\theta(x^{(i)}))]}}</script><p>注意：不惩罚第一项$\theta_0$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costReg</span><span class="params">(theta, X, Y, lambuda=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 不惩罚第一项</span></span><br><span class="line">    _theta = theta[<span class="number">1</span>: ]</span><br><span class="line">    reg = (lambuda / (<span class="number">2</span> * len(X))) *(_theta @ _theta)  <span class="comment"># _theta@_theta == inner product</span></span><br><span class="line">    <span class="keyword">return</span> computeCost(theta, X, Y) + reg</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里因为做特征映射的时候已经添加了偏置项，所以不用手动添加了。</span></span><br><span class="line">X = _data2.values  </span><br><span class="line">Y = data2[<span class="string">'Accepted'</span>].values</span><br><span class="line">theta = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">print(costReg(theta,X,Y,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>0.6931471805599454</code></pre><h2 id="正则化梯度"><a href="#正则化梯度" class="headerlink" title="正则化梯度"></a>正则化梯度</h2><p>正则化梯度下降：</p><script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}{x_0}^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m[{({h_\theta(x^{(i)})}-y^{(i)})}{x_j}^{(i)}+\frac{\lambda}{m}\theta_j],for j=1,2,...n</script><p>对上面的算法中 j=1,2,…,n 时的更新式子进行调整可得：</p><script type="math/tex; mode=display">\theta_j:=\theta_j(1-a\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}{x_j}^{(i)}</script><p>仅需要计算出梯度：<script type="math/tex">\frac{\partial}{\partial\theta_j}J(\theta)</script></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientReg</span><span class="params">(theta, X, Y, lambuda=<span class="number">1</span>)</span>:</span></span><br><span class="line">    reg = (lambuda / len(X)) * theta</span><br><span class="line">    reg[<span class="number">0</span>] = <span class="number">0</span>  </span><br><span class="line">    <span class="keyword">return</span> gradient(theta, X, Y) + reg</span><br><span class="line"></span><br><span class="line">print(gradientReg(theta,X,Y,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>[8.47457627e-03 1.87880932e-02 7.77711864e-05 5.03446395e-02 1.15013308e-02 3.76648474e-02 1.83559872e-02 7.32393391e-03 8.19244468e-03 2.34764889e-02 3.93486234e-02 2.23923907e-03 1.28600503e-02 3.09593720e-03 3.93028171e-02 1.99707467e-02 4.32983232e-03 3.38643902e-03 5.83822078e-03 4.47629067e-03 3.10079849e-02 3.10312442e-02 1.09740238e-03 6.31570797e-03 4.08503006e-04 7.26504316e-03 1.37646175e-03 3.87936363e-02]</code></pre><h2 id="运用Scipy优化算法来最小化代价函数，学习参数"><a href="#运用Scipy优化算法来最小化代价函数，学习参数" class="headerlink" title="运用Scipy优化算法来最小化代价函数，学习参数"></a>运用Scipy优化算法来最小化代价函数，学习参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result2 = opt.fmin_tnc(func=costReg, x0=theta, fprime=gradientReg, args=(X, Y, <span class="number">2</span>))</span><br><span class="line">print(result2[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>[ 0.90267454  0.33721089  0.76006404 -1.39757946 -0.51417075 -0.91389985  0.01516214 -0.21926017 -0.22677642 -0.16219637 -1.01270257 -0.04169398 -0.39984069 -0.14458017 -0.82296284 -0.20346048 -0.13186937 -0.04837714 -0.17183934 -0.17077936 -0.38820995 -0.72773035  0.00607685 -0.19391899  0.00314606 -0.21203169 -0.06947222 -0.69320886]</code></pre><h2 id="输出预测值及被录取（输出值为1）的概率-1"><a href="#输出预测值及被录取（输出值为1）的概率-1" class="headerlink" title="输出预测值及被录取（输出值为1）的概率"></a>输出预测值及被录取（输出值为1）的概率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">final_theta = result2[<span class="number">0</span>]</span><br><span class="line">predictions = predict(final_theta, X)</span><br><span class="line">correct = [<span class="number">1</span> <span class="keyword">if</span> a==b <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, Y)]</span><br><span class="line">accuracy = sum(correct) / len(correct)</span><br><span class="line"><span class="comment">#输出预测精度</span></span><br><span class="line">print(accuracy)</span><br></pre></td></tr></table></figure><pre><code>0.8305084745762712</code></pre><h3 id="高级Python库scikit-learn"><a href="#高级Python库scikit-learn" class="headerlink" title="高级Python库scikit-learn"></a>高级Python库scikit-learn</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model<span class="comment">#调用sklearn的线性回归包</span></span><br><span class="line">model = linear_model.LogisticRegression(penalty=<span class="string">'l2'</span>, C=<span class="number">1.0</span>)</span><br><span class="line">model.fit(X, Y.ravel())</span><br><span class="line">print(model.score(X,Y))</span><br></pre></td></tr></table></figure><pre><code>0.8305084745762712</code></pre><h2 id="绘制决策边界-1"><a href="#绘制决策边界-1" class="headerlink" title="绘制决策边界"></a>绘制决策边界</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-1</span>, <span class="number">1.5</span>, <span class="number">250</span>)</span><br><span class="line">xx, yy = np.meshgrid(x, x)</span><br><span class="line"></span><br><span class="line">z = feature_mapping(xx.ravel(), yy.ravel(), <span class="number">6</span>).values</span><br><span class="line">z = z @ final_theta</span><br><span class="line">z = z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">plt.scatter(positive[<span class="string">'Test 1'</span>], positive[<span class="string">'Test 2'</span>], s=<span class="number">50</span>, c=<span class="string">'b'</span>, marker=<span class="string">'o'</span>, label=<span class="string">'Accepted'</span>)</span><br><span class="line">plt.scatter(negative[<span class="string">'Test 1'</span>], negative[<span class="string">'Test 2'</span>], s=<span class="number">50</span>, c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>, label=<span class="string">'Rejected'</span>)</span><br><span class="line">plt.contour(xx, yy, z, <span class="number">0</span>)</span><br><span class="line">plt.ylim(<span class="number">-.8</span>, <span class="number">1.2</span>)</span><br></pre></td></tr></table></figure><pre><code>(-0.8, 1.2)</code></pre><p><img src="/images/8.5_4.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在这部分练习中，你将建立一个logistics回归模型来预测一个学生是否能被大学录取。假如你是大学招生办的工作人员，你想通过学生的两次考试成绩来决定他被录取的概率。你有一些往届学生的历史数据作为逻辑回归的训练集，对于每一个训练样本，你有学生两次考试的分数和录取结果。你的任务
      
    
    </summary>
    
      <category term="python" scheme="https://hiworldalice.github.io/categories/python/"/>
    
    
      <category term="机器学习" scheme="https://hiworldalice.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达机器学习作业Python实现(一)：线性回归</title>
    <link href="https://hiworldalice.github.io/2019/07/29/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>https://hiworldalice.github.io/2019/07/29/线性回归/</id>
    <published>2019-07-28T19:20:34.000Z</published>
    <updated>2019-07-30T05:42:03.977Z</updated>
    
    <content type="html"><![CDATA[<h2 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h2><p>在本部分的练习中，您将使用一个变量实现线性回归，以预测食品卡车的利润：<br>假设你是一家餐馆的首席执行官，正在考虑不同的城市开设一个新的分店。该连锁店已经在各个城市拥有卡车，而且你有来自城市的利润和人口数据。<br>您希望使用这些数据来帮助您选择将哪个城市扩展到下一个城市。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h3 id="导入数据集并查看："><a href="#导入数据集并查看：" class="headerlink" title="导入数据集并查看："></a>导入数据集并查看：</h3><p>注意：开始任何任务之前，通过可视化来理解数据通常是有用的。在这个数据集中，只有两个属性（利润和人口），可以绘制散点图。许多其他问题是多维度的，例如下面的多变量线性回归的例子，就不能再二维图上画出来。</p><p>.describe()显示各统计值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">r'C:/Users/DH/Desktop/ex1data1.txt'</span></span><br><span class="line">data = pd.read_csv(path,names=[<span class="string">'Population'</span>,<span class="string">'Profit'</span>])</span><br><span class="line">print(data.describe())</span><br></pre></td></tr></table></figure><pre><code>       Population     Profitcount   97.000000  97.000000mean     8.159800   5.839135std      3.869884   5.510262min      5.026900  -2.68070025%      5.707700   1.98690050%      6.589400   4.56230075%      8.578100   7.046700max     22.203000  24.147000</code></pre><p>绘制散点图：<br>plt.scatter(x,y)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(data[<span class="string">'Population'</span>], data[<span class="string">'Profit'</span>])</span><br><span class="line">plt.xlabel(<span class="string">'Population'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Profit'</span>)</span><br><span class="line"><span class="comment">#data.plot(kind='scatter', x='Population', y='Profit', figsize=(8,5))</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/output_6_0.png" alt="png"></p><h3 id="实现代价函数表达式，为所需各变量赋值"><a href="#实现代价函数表达式，为所需各变量赋值" class="headerlink" title="实现代价函数表达式，为所需各变量赋值"></a>实现代价函数表达式，为所需各变量赋值</h3><p>代价函数表达式：<script type="math/tex">J(\theta_0,\theta_1,...,\theta_n)=\frac{\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}^2}{2m}</script>其中<script type="math/tex">h_\theta=\theta^TX=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n</script></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X,Y,theta)</span>:</span></span><br><span class="line">    inner = np.power((X*theta.T)-Y,<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner)/(<span class="number">2</span>*len(X))</span><br></pre></td></tr></table></figure><p>数据集插入一列方便之后进行向量运算，作为$x_0$,值均为1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.insert(<span class="number">0</span>,<span class="string">'Ones'</span>,<span class="number">1</span>)</span><br><span class="line">print(data.head())</span><br></pre></td></tr></table></figure><pre><code>   Ones  Population   Profit0     1      6.1101  17.59201     1      5.5277   9.13022     1      8.5186  13.66203     1      7.0032  11.85404     1      5.8598   6.8233</code></pre><p>对表达式中自变量X，因变量Y，未知参数θ初始化<br>（1）data.shape[1]表示列数，data.shape[0]表示行数。<br>（2）data.iloc[]函数通过行号来取行数据<br>     data.iloc[:,cols-1:cols]提取最后一列数据，从0开始计数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#set X(training data) and Y(target varible)</span></span><br><span class="line">cols = data.shape[<span class="number">1</span>] <span class="comment">#列数</span></span><br><span class="line">X = data.iloc[:,<span class="number">0</span>:cols<span class="number">-1</span>] </span><br><span class="line">Y = data.iloc[:,cols<span class="number">-1</span>:cols] <span class="comment">#取最后一列，即目标向量</span></span><br></pre></td></tr></table></figure><p>将X,Y转换为矩阵，初始化theta为一个（1,2）矩阵：<br>转换为矩阵是为了方便矩阵运算，对于矩阵matrix，<em>即为点乘，而对于数组array，</em>为对应位置元素相乘，若要点乘，则需要用到np.dot(X1,X2)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = np.matrix(X.values)</span><br><span class="line">Y = np.matrix(Y.values)</span><br><span class="line">theta =np.matrix([<span class="number">0</span>,<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h3 id="实现梯度下降函数"><a href="#实现梯度下降函数" class="headerlink" title="实现梯度下降函数"></a>实现梯度下降函数</h3><p>bathch gradient decent(批量梯度下降)</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{partial\theta_j}J(\theta)</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}{x_j}^{(i)}}{m}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, Y, theta, alpha, epoch)</span>:</span></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))  <span class="comment"># 初始化一个临时矩阵(1, 2)</span></span><br><span class="line">    cost = np.zeros(epoch)                   <span class="comment"># 初始化一个ndarray，包含每次epoch的cost</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]                           <span class="comment"># m为总的样本数</span></span><br><span class="line">    <span class="comment">#利用向量化一步求解</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        temp = theta - (alpha / m) * (X * theta.T - Y).T * X</span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = computeCost(X,Y,theta) <span class="comment">#得到每次迭代代价函数的值</span></span><br><span class="line">    <span class="keyword">return</span> theta,cost</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化学习率α和要进行迭代的次数</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">epoch = <span class="number">1000</span></span><br><span class="line"><span class="comment">#现在让我们运行梯度下降算法来将我们的参数θ适合于训练集</span></span><br><span class="line">final_theta,cost = gradientDescent(X,Y,theta,alpha,epoch)</span><br></pre></td></tr></table></figure><h3 id="绘制线性模型，直观看出拟合"><a href="#绘制线性模型，直观看出拟合" class="headerlink" title="绘制线性模型，直观看出拟合"></a>绘制线性模型，直观看出拟合</h3><p>（1）np.linspace()在指定的间隔内返回均匀间隔的数字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(data.Population.min(),data.Population.max(),<span class="number">100</span>)</span><br><span class="line">f = final_theta[<span class="number">0</span>,<span class="number">0</span>] + (final_theta[<span class="number">0</span>,<span class="number">1</span>] * x) </span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>,<span class="number">4</span>))</span><br><span class="line">ax.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">ax.scatter(data[<span class="string">'Population'</span>], data.Profit, label=<span class="string">'Traning Data'</span>)</span><br><span class="line">ax.legend(loc=<span class="number">2</span>)  <span class="comment"># 2表示在左上角</span></span><br><span class="line">ax.set_xlabel(<span class="string">'Population'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Predicted Profit vs. Population Size'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/output_18_0.png" alt="png"></p><p>由于梯度方程式函数也在每个训练迭代中输出一个代价的向量，所以我们也可以绘制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig,ax = plt.subplots(figsize = (<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line">ax.plot(np.arange(epoch),cost,<span class="string">'r'</span>) <span class="comment">#np.arange()返回等差数组</span></span><br><span class="line">ax.set_xlabel(<span class="string">'Iterations'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Cost'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Error vs. Training Epoch'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/output_20_0.png" alt="png"></p><h4 id="正规方程法"><a href="#正规方程法" class="headerlink" title="正规方程法"></a>正规方程法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span><span class="params">(X, y)</span>:</span></span><br><span class="line">    theta = np.linalg.inv(X.T*X)*X.T*y<span class="comment">#X.T*X等价于X.T.dot(X)</span></span><br><span class="line">    <span class="keyword">return</span> theta.T</span><br><span class="line">final_theta2 = normalEqn(X,Y)</span><br></pre></td></tr></table></figure><p>打印两种方法的得到的参数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(final_theta,<span class="string">'\n'</span>,final_theta2)</span><br></pre></td></tr></table></figure><pre><code>[[-3.24140214  1.1272942 ]]  [[-3.89578088  1.19303364]]</code></pre><h2 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h2><p>练习1还包括一个房屋价格数据集，其中有2个变量（房子的大小，卧室的数量）和目标（房子的价格）。 我们使用我们已经应用的技术来分析数据集。</p><h3 id="导入数据集并查看"><a href="#导入数据集并查看" class="headerlink" title="导入数据集并查看"></a>导入数据集并查看</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">r'C:/Users/DH/Desktop/ex1data2.txt'</span></span><br><span class="line">data = pd.read_csv(path,names = [<span class="string">'sizes'</span>,<span class="string">'rooms'</span>,<span class="string">'prices'</span>])</span><br><span class="line">print(data.head())</span><br></pre></td></tr></table></figure><pre><code>   sizes  rooms  prices0   2104      3  3999001   1600      3  3299002   2400      3  3690003   1416      2  2320004   3000      4  539900</code></pre><p>对于此任务，我们添加了另一个预处理步骤：特征缩放，否则以$\theta_1$，$\theta_2$两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。<br>解决的方法是尝试将所有特征的尺度都尽量缩放到-1 到 1 之间:</p><script type="math/tex; mode=display">\frac{x_n-\mu_n}{s_n}</script><p>其中$\mu_n$是平均值，$s_n$是标准差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'sizes'</span>] = (data[<span class="string">'sizes'</span>] - data[<span class="string">'sizes'</span>].mean())/data[<span class="string">'sizes'</span>].std()</span><br><span class="line">data[<span class="string">'rooms'</span>] = (data[<span class="string">'rooms'</span>] - data[<span class="string">'rooms'</span>].mean())/data[<span class="string">'rooms'</span>].std()</span><br><span class="line">data[<span class="string">'prices'</span>] = (data[<span class="string">'prices'</span>] - data[<span class="string">'prices'</span>].mean())/data[<span class="string">'prices'</span>].std()</span><br><span class="line">print(data.head())</span><br></pre></td></tr></table></figure><pre><code>      sizes     rooms    prices0  0.130010 -0.223675  0.4757471 -0.504190 -0.223675 -0.0840742  0.502476 -0.223675  0.2286263 -0.735723 -1.537767 -0.8670254  1.257476  1.090417  1.595389</code></pre><h3 id="实现代价函数表达式，为所需各变量赋值-1"><a href="#实现代价函数表达式，为所需各变量赋值-1" class="headerlink" title="实现代价函数表达式，为所需各变量赋值"></a>实现代价函数表达式，为所需各变量赋值</h3><p>此步骤除代价函数表达式与单变量线性回归有所不同，参数个数增加外，处理方式与单变量线性回归相同。<br>代价函数表达式：<script type="math/tex">J(\theta_0,\theta_1,...,\theta_n)=\frac{\sum_{i=1}^m{({h_\theta(x^{(i)})}-y^{(i)})}^2}{2m}</script>其中<script type="math/tex">h_\theta=\theta^TX=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n</script></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">theta = np.matrix(np.zeros(<span class="number">3</span>))</span><br><span class="line"><span class="comment">#插入一列，作为x0,值为1</span></span><br><span class="line">data.insert(<span class="number">0</span>,<span class="string">'ones'</span>,<span class="number">1</span>)</span><br><span class="line">cols = data.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#X = np.matrix(data[:2]) 仅选取前两行</span></span><br><span class="line">X = np.matrix(data.iloc[:,<span class="number">0</span>:cols<span class="number">-1</span>])</span><br><span class="line">Y = np.matrix(data.iloc[:,cols<span class="number">-1</span>:cols])</span><br><span class="line"><span class="comment">#代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X,Y,theta)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> np.sum(np.power((X*theta.T) - Y,<span class="number">2</span>))/(<span class="number">2</span>*m)</span><br></pre></td></tr></table></figure><h3 id="实现梯度下降算法"><a href="#实现梯度下降算法" class="headerlink" title="实现梯度下降算法"></a>实现梯度下降算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X,Y,theta,alpha,epoch)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    cost = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">        theta = theta - alpha/m*(X*theta.T - Y).T*X</span><br><span class="line">        cost.append(computeCost(X,Y,theta))       <span class="comment">#列表添加元素</span></span><br><span class="line">    <span class="keyword">return</span> theta,cost</span><br><span class="line"></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">epoch = <span class="number">1000</span></span><br><span class="line">final_theta,cost = gradientDescent(X,Y,theta,alpha,epoch)</span><br><span class="line">final_cost = computeCost(X,Y,final_theta)</span><br><span class="line">print(final_cost)</span><br></pre></td></tr></table></figure><pre><code>0.13070336960771892</code></pre><h3 id="画图查看训练过程"><a href="#画图查看训练过程" class="headerlink" title="画图查看训练过程"></a>画图查看训练过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#画图查看训练过程</span></span><br><span class="line">fig,ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.plot(np.arange(epoch),cost,<span class="string">'r'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Iterations'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Cost'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Error vs. Training Epoch'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/output_34_0.png" alt="png"></p><h4 id="正规方程法-1"><a href="#正规方程法-1" class="headerlink" title="正规方程法"></a>正规方程法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正规方程法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span><span class="params">(X, y)</span>:</span></span><br><span class="line">    theta = np.linalg.inv(X.T*X)*X.T*y<span class="comment">#X.T*X等价于X.T.dot(X)</span></span><br><span class="line">    <span class="keyword">return</span> theta.T</span><br><span class="line">final_theta2 = normalEqn(X,Y)</span><br></pre></td></tr></table></figure><p>打印两种方法的得到的参数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(final_theta,<span class="string">'\n'</span>,final_theta2)</span><br></pre></td></tr></table></figure><pre><code>[[-1.11113782e-16  8.78503652e-01 -4.69166570e-02]]  [[-1.17961196e-16  8.84765988e-01 -5.31788197e-02]]</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;单变量线性回归&quot;&gt;&lt;a href=&quot;#单变量线性回归&quot; class=&quot;headerlink&quot; title=&quot;单变量线性回归&quot;&gt;&lt;/a&gt;单变量线性回归&lt;/h2&gt;&lt;p&gt;在本部分的练习中，您将使用一个变量实现线性回归，以预测食品卡车的利润：&lt;br&gt;假设你是一家餐馆的首席
      
    
    </summary>
    
      <category term="python" scheme="https://hiworldalice.github.io/categories/python/"/>
    
    
      <category term="机器学习" scheme="https://hiworldalice.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://hiworldalice.github.io/2019/07/28/hello-world/"/>
    <id>https://hiworldalice.github.io/2019/07/28/hello-world/</id>
    <published>2019-07-28T09:29:14.049Z</published>
    <updated>2019-07-28T15:45:33.951Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
